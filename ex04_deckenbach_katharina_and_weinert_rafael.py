# -*- coding: utf-8 -*-
"""Ex04_Deckenbach_Katharina_and_Weinert_Rafael.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZTlDh70hhFDxU-TCJ06CT3H5ZtTE5f22

<a href="https://colab.research.google.com/github/rafaelweinert/DD1420_ML_Excercises/blob/main/DD1420HT23_Ex4_STUDENT_VERSION_v1.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import torch
import torchvision
import torchvision.transforms as T
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import math
from typing import List
from types import SimpleNamespace
from tqdm import tqdm

"""# Exercise 4 - ML & Neural Networks  (DD1420 HT23)

## Instructions

This Jupyter Notebook contains the exercises for DD1420 **Module 4 ML & Neural Networks**. There are a several problems in this exercise, each broken into numbered subproblems, *e.g.* 1.1.1. The points for each problem are marked *e.g.* $\color{red}{\text{(} x \text{ points)}}$. In total, the exercise is worth **12** points.

<br>

Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the Code of Conduct. In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions. If you need assistance with the exercise, you are encouraged to post a question to the appropriate Discussion Topic or sign up for a help session.

<br>

Make sure you fill in any place that says `YOUR CODE HERE` or "YOUR ANSWER HERE" and delete `raise NotImplementedError()` once you have implemented the solution.

<br>

You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.

<br>

**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all`. Then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows

<br>

`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`

<br>

where you replace `??` with the correct exercise number. If you are working alone you do not need to include a partner name.

<br>

Good luck!


#Name
**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly):

<br>

### Katharina Deckenbach

### Rafael Weinert

# 4.1 Training a neural network with backpropagation  (12 points)

In this exercise we will once again train a machine learning algorithm to classify handwritten digits from MNISt, although this time we will use a bigger version with $64 \times 64$ images and 60,000 examples.

<br>

We will make use of [PyTorch](https://pytorch.org/) which is a popular machine learning framework for developing neural networks. You will not be expected to write PyTorch-specific code in this exercise, just standard numpy and Python. You won't need to follow along with what PyTorch is doing - we try to explain the important bits.

<br>

The first thing we will do is load the MNIST handwritten digits dataset,
"""

# these are pytorch's dataset class. it downloads and sets up the dataset
mnist_train = torchvision.datasets.MNIST(root="data",
                                         train=True,
                                         download=True,
                                         transform=T.Compose([T.ToTensor(),
                                                              T.Lambda(lambda x: torch.flatten(x)),
                                                              ]),
                                         )
mnist_test = torchvision.datasets.MNIST(root="data",
                                        train=False,
                                        download=True,
                                        transform=T.Compose([T.ToTensor(),
                                                             T.Lambda(lambda x: torch.flatten(x)),
                                                             ]),
                                        )

"""We use something called a `dataloader` class from PyTorch, which handles feeding data from the dataset to the network. It does things like shuffle the data and yield samples.


"""

# these are pytorch's dataloader class. they shuffle and yield sample points
# for us. you don't have to code data manipulations yourself in this exercise
train_loader = DataLoader(mnist_train, batch_size=1, shuffle=True)
val_loader = DataLoader(mnist_test, batch_size=1, shuffle=True)

"""Next we will define the size of our neural network, which will have $L = 3$ layers of width $m_{\ell=\{1,\dots,L\}} = \{784, 30, 10\}$"""

# hyperparameters. sets up the network dimensions
# how many nodes in each
layers_list = [784, 30, 10]
L = len(layers_list)

"""We will define a function that will rescale our random weights to a distribution that has been proven to be a better initialization than just a standard `randn` distribution. The Xavier initialization simply divides the randomly initialized weights of each layer by the square root of the input size $\sqrt{k}$."""

def initialization_scaling(weight_matrix):
  """
  This implements Xavier initialization. It helps training of deep networks.
  """
  input_size = weight_matrix.shape[0]
  return weight_matrix / math.sqrt(input_size)

"""**4.1.1** *Complete the function provided below that creates a single layer of a neural network. Given the input size and the ouptut size this function should create the weight matrix and bias vector with random initialization. If the `scaled_init` flag is `True` it should apply the Xavier scaled defined above. The bias vector should be initialized to zero.* $\color{red}{\text{(1 point)}}$"""

def create_single_layer(input_size: int, output_size: int, scaled_init=False):
  """
  Create a single layer
  output: a layer object. weights and biases are in layer.weight and layer.bias

  Usage:
    layer = create_single_layer(784, 10)
    layer_weight_matrix = layer.weight
    layer_bias_vector = layer.bias
  """
  bias_vector = np.random.randn(output_size)
  weight_matrix = np.random.randn(output_size, input_size)
  if scaled_init:
    weight_matrix = initialization_scaling(weight_matrix)
    bias_vector = np.zeros((output_size,1))

  # The output being a SimpleNamespace is just a coding convenience. It
  # emulates how we access the weights and biases in pytorch.
  # without this we would have to access weights using layers[0][0],
  # with this we can do layers[0].weight so that it is a bit more readable
  return SimpleNamespace(weight=weight_matrix, bias=bias_vector)

"""**4.1.2** *Complete the function provided below that creates all the layers of the neural network using the function you created in 4.1.1. The `layers_list` contains a list of layer widths for each layer. Loop through that list, create each layer, and append it to a `layers` list. The function should output `layers`, a list of initialized layers.* $\color{red}{\text{(1 point)}}$"""

def create_layers(layers_list: List):
  """
  Creates the weights and biases if layers according to the given list
  output: a list of layers

  Usage:
    layers = create_layers(layers_list)
    some_layer = layers[np.random.randint(L)]
  """

  #To match the Lecture notation we start with idx=1, i.e. there is no W^(0) or b^(0)
  layers = [None]
  for idx in range(L-1):
      layer = create_single_layer(layers_list[idx],layers_list[idx+1],True)
      layers.append(layer)
  return layers

"""Next, we provide you with code for the logistic sigmoid activation function `sigmoid` and its derivative `sigmoid_prime` which should be applied to pre-activations."""

def sigmoid(z):
  """
  The sigmoid function
  Usage:
    activation_output = sigmoid(some_input)
  """
  return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
  """
  $\sigma\prime$ in the lecture notes
  """
  return sigmoid(z)*(1-sigmoid(z))

"""**4.1.3** *Complete the following functions that the define the MSE loss (the loss for a single sample $\ell_{MSE}$) and its derivative.* $\color{red}{\text{(1 point)}}$

Note that this is not a very good choice of a loss for classification, but the derivative is easier to calculate and code and it will give reasonable results on MNIST.
"""

def loss_fn(y_hat, y):
  """
  This is the MSE loss for a single sample.
  """
  return np.square(y - y_hat)


def dloss_dyhat(y_hat, y):
  """
  Gradient of the MSE loss for a single sample.
  """
  result = np.full((1,y.shape[0]), np.mean((-(2 * (y - y_hat)))))
  #return (-(2 * (y - y_hat))).T
  return result
"""Next, we will start to define some of the essential functions for training the neural network. We will start with a function for the pre-activations $z^{(\ell)}$ and the activations $a^{(\ell)}$.

**4.1.4**.  *Complete the function below that returns the preactivation given the layers weight matrix $W^{(\ell)}$, bias vector $b^{(\ell)}$ and the previous layers activation $a^{(\ell-1)}$.* $\color{red}{\text{(1 point)}}$

hint: [The @ operator can be used as a shorthand for np.matmul on ndarrays.](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)
"""

def calculate_preactivation(layer_weight, layer_bias, previous_activation):
  #print(layer_weight.shape)
  #print(layer_bias.shape)
  #print(previous_activation.shape)
  z = layer_weight @ previous_activation + layer_bias

  return z

"""A quick sanity check."""

W = np.array([[2,-2], [-3,-1], [1, -3]])
b = np.array([[-1], [0], [0]])
a = np.array([[2],[-2]])
calculate_preactivation(W,b,a)

"""**4.1.5**.  *Complete the function below that returns the activation of a layer given the preactivation $z^{(\ell)}$.* $\color{red}{\text{(1 point)}}$"""

def calculate_activation(z):
  return sigmoid(z)

"""A quick sanity check."""

z = np.array([[-1], [0], [1]])
calculate_activation(z)

"""**4.1.6.** *Write a function that computes and returns the gradient of the last layer $g^{(L)}$. You should make use of some previous functions you have written. You may need to use `.squeeze()` to ensure dimensions match.* $\color{red}{\text{(1 point)}}$"""

def calculate_g_L(y, a, z):
  dy_hat = dloss_dyhat(a, y)
  return dy_hat @ np.diag(sigmoid_prime(z).squeeze())

"""A quick sanity check.

"""

y = np.ones((5,1))
a = np.zeros((5,1))
z = np.ones((5,1))
calculate_g_L(y,a,z)

"""**4.1.7.** *Write a function that computes and returns the gradient of an aribtrary layer $g^{(\ell)}$ (except $L$). You may need to use `.squeeze()` to ensure dimensions match.* $\color{red}{\text{(1 point)}}$"""

def calculate_g_l(g_l_previous, layer_weight_previous, z):
  g_l = g_l_previous @ layer_weight_previous @ np.diag(sigmoid_prime(z).squeeze())
  return g_l

"""A quick sanity check.

"""

y = np.ones((5,1))
a = np.zeros((5,1))
z = np.ones((5,1))
g_L = calculate_g_L(y,a,z)
W = np.ones((5,5))
z = np.ones((5,1))
g_l = calculate_g_l(g_L,W,z)
g_l

"""**4.1.8.** *Write a function that computes and returns the gradient of the loss w.r.t. the weights $\nabla_{W^{(\ell)}} J$ given the gradient $g^{(\ell)}$ and the activations from the next layer $a^{(\ell-1)}$.* $\color{red}{\text{(1 point)}}$"""

def calculate_nabla_weight(g_l, a_next):
  return g_l.T @ a_next.T

"""A quick sanity check."""

g_l = np.ones((1,5))
a_next = np.ones((3,1))
nabW = calculate_nabla_weight(g_l, a_next)
nabW

"""**4.1.9.** *Write a function that computes and returns the gradient of the loss w.r.t. the bias $\nabla_{b^{(\ell)}} J$ given the gradient $g^{(\ell)}$ and the activations from the next layer $a^{(\ell-1)}$.* $\color{red}{\text{(1 point)}}$"""

def calculate_nabla_bias(g_l):
  return g_l.T

"""A quick sanity check"""

g_l = np.ones((1,5))
nabb = calculate_nabla_bias(g_l)
nabb

"""Great! We are finally ready to put everything together and train the neural network. We have provided most of the code for you.

<br>

**4.1.10.** *Insert the functions you have defined in **4.1.2** and **4.1.5** through **4.1.9** in the correct places in the following code. Then run the code to train the neural network to classify handwritten digits.* $\color{red}{\text{(3 points)}}$
"""

# initialization = create the layers
layers = create_layers(layers_list)
lr = 3.0
# this is one epoch
# ====== loop through 1 epoch using the data loader =====

for (x, y) in tqdm(train_loader, desc=f'Epoch progress'):
  # we used pytorch loaders, convert back to numpy and proper shape
  x = x.numpy().T
  # our network uses one_hot encoding to classify the digit so
  # we convert the target to a one_hot encoding
  y = torch.nn.functional.one_hot(y,num_classes=10).numpy().T
  # actual updates occur here
  activation = x
  # set up the variables for saving the a & z values
  # a^(0) is set to x
  activations = [activation]
  # z^(0) does not exist so we store a None for it
  pre_activations = [None]

  # ====== do forward propagation =====
  # looping through the layers, computing and storing activations & pre-activations
  for layer in layers[1:]: #since layers[0] does not exist/is None

    pre_activation = calculate_preactivation(layer.weight, layer.bias, activations[-1])
    activation = sigmoid(pre_activation)
    # store these for the backprop
    pre_activations.append(pre_activation)
    activations.append(activation)
    #print('activation', activation)

  # ===== do backpropagation =====
  # setting up record keeping of the gradients, nabla_weights and nabla_biases
  # note that we have no gradients for W^(0) since we index starting at 1
  g_ls = [None]
  nabla_weights = [None]
  nabla_biases = [None]
  g_ls.extend([np.zeros((1, m)) for m in layers_list] )
  nabla_weights.extend([np.zeros_like(layer.weight) for layer in layers[1:]])
  nabla_biases.extend([np.zeros_like(layer.bias) for layer in layers[1:]])

  # looping through the layers, compute gradients and nabla's
  for l in range(L-1, 0, -1):

    # g^L is calculated differently, so we check and handle that first
    if l == L-1:
      g_ls[l] = calculate_g_L(y,activations[l],pre_activations[l])
    else:
      g_ls[l] = calculate_g_l(g_ls[l+1], nabla_weights[l+1], pre_activations[l])
    nabla_weights[l] = calculate_nabla_weight(g_ls[l], activations[l-1])
    nabla_biases[l] = calculate_nabla_bias(g_ls[l])
  # update the weights with the lr according to SGD algorithm
  for l in range(L-1, 0, -1):
    #if l == 1:
      #print(layers[l].weight)
      #print(nabla_weights[l][0][0])
    layers[l].weight = layers[l].weight - lr * nabla_weights[l]
    layers[l].bias = layers[l].bias - lr * nabla_biases[l]

    #if l == 1:
      #print(layers[l].weight)

"""Run the following code to make a forward pass over the test to see how it performed."""

# create a forward function for evaluation, without backprop
def forward(layers, x):
  """
  Forward propagation.
  """
  activation = x
  for layer in layers[1:]: #since layers[0] does not exist
    activation = sigmoid(layer.weight @ activation +layer.bias)
  return activation

# Evaluate
test_results = []
for (x,y) in val_loader:
  x = x.numpy().T
  y = y.numpy().T
  test_results.append((np.argmax(forward(layers, x)), y))

accuracy = sum(int(x == y) for (x, y) in test_results)/len(test_results)*100
print(accuracy)

"""And we can visualize some of our predictions with the code below"""

# and show an example
x, y = next(iter(val_loader))
x = x.numpy().T
y = y.numpy().T
plt.title(f'Prediction is {np.argmax(forward(layers, x))}')
plt.imshow(x.reshape(28,28), cmap='gray');

