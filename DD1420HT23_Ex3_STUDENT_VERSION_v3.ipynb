{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafaelweinert/DD1420_ML_Excercises/blob/main/DD1420HT23_Ex3_STUDENT_VERSION_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d89c87",
      "metadata": {
        "id": "47d89c87"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "#! Required python packages: numpy, scipy, scikit-learn,\n",
        "# matplotlib, seaborn, pandas, tqdm, ipywidgets\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn as sns; sns.set()\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "np.random.seed(17)\n",
        "plt.rcParams['figure.figsize'] = [9.5, 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144080e8",
      "metadata": {
        "id": "144080e8"
      },
      "source": [
        "# Exercise 3 - ML & Generalization  (DD1420 HT23)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This Jupyter Notebook contains the exercises for DD1420 **Module 3 ML & Generalization**. There are a several problems in this exercise, each broken into numbered subproblems, *e.g.* 1.1.1. The points for each problem are marked *e.g.* $\\color{red}{\\text{(} x \\text{ points)}}$. In total, the exercise is worth **23** points.\n",
        "\n",
        "<br>\n",
        "\n",
        "Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the Code of Conduct. In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions. If you need assistance with the exercise, you are encouraged to post a question to the appropriate Discussion Topic or sign up for a help session.\n",
        "\n",
        "<br>\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and delete `raise NotImplementedError()` once you have implemented the solution.\n",
        "\n",
        "<br>\n",
        "\n",
        "You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all`. Then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows\n",
        "\n",
        "<br>\n",
        "\n",
        "`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`\n",
        "\n",
        "<br>\n",
        "\n",
        "where you replace `??` with the correct exercise number.\n",
        "\n",
        "<br>\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wLs7mE3KolE7",
      "metadata": {
        "id": "wLs7mE3KolE7"
      },
      "source": [
        "![h.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb38313",
      "metadata": {
        "id": "abb38313"
      },
      "source": [
        "#Name\n",
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly):\n",
        "\n",
        "<br>\n",
        "\n",
        "### Student 1\n",
        "\n",
        "### Student 2  (if you work with a partner)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef3065c",
      "metadata": {
        "id": "4ef3065c"
      },
      "source": [
        "# 3.1 Generalization of an Image Classifier  (13 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242ca890",
      "metadata": {
        "id": "242ca890"
      },
      "source": [
        "Let us start with a small experiment. We use the most famous dataset in the machine learning world -- [**MNIST**](https://en.wikipedia.org/wiki/MNIST_database). This full dataset contains 60,000 digits (0-9) handwritten written by high school students and employees of the United States Census Bureau. We will use a subset of MNIST with smaller image size. The goal is to classify each example according to the visible number. Because we are dealing with more than 2 classes (binary classification) we call this type of problem *multi-class* classification.\n",
        "\n",
        "<br>\n",
        "\n",
        "Note that any functions you write you can reuse unless explicitly instructed not to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d30219",
      "metadata": {
        "id": "a9d30219"
      },
      "source": [
        "The images can be loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "469478fb",
      "metadata": {
        "id": "469478fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "data = load_digits()\n",
        "imgs = data['images'] # shape (1797, 8, 8)\n",
        "targets = data['target'] # shape (1797,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ddb9b4f",
      "metadata": {
        "id": "4ddb9b4f"
      },
      "source": [
        "A subset of the images look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc86c553",
      "metadata": {
        "id": "dc86c553"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = 8, 10\n",
        "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=\"all\", sharey=\"all\")\n",
        "for j in range(ncols):\n",
        "    for i in range(nrows):\n",
        "        ax[i][j].imshow(imgs[ncols * i + j], cmap=plt.cm.gray)\n",
        "        ax[i][j].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e94d8586",
      "metadata": {
        "id": "e94d8586"
      },
      "source": [
        "For the purposes of this exercise we are going to flatten each $8 \\times 8$ image into a **vector** of length $64$. Also, to keep the computational load reasonable we will only use `num_samples = 350`of the 1,797 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67a2562",
      "metadata": {
        "id": "f67a2562"
      },
      "outputs": [],
      "source": [
        "num_samples = 350\n",
        "xs = np.reshape(imgs, newshape=[-1, 8 * 8]) # shape (1797, 64)\n",
        "ys = targets # shape (1797,)\n",
        "\n",
        "ix = np.random.choice(len(xs), size=num_samples, replace=False)\n",
        "xs = xs[ix] # shape (num_samples, 64)\n",
        "ys = ys[ix] # shape (num_samples,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc58d01",
      "metadata": {
        "id": "cfc58d01"
      },
      "source": [
        "We provide a function to split the dataset randomly into two non-overlapping datasets as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f799bca",
      "metadata": {
        "id": "3f799bca"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "def split_into_subsets(X: np.ndarray, Y: np.ndarray, sizes: Tuple):\n",
        "    \"\"\"\n",
        "    Splits two np.ndarrays randomly into non-overlapping np.ndarrays\n",
        "    according to some given percentages of the data (expressed as floats)\n",
        "    \"\"\"\n",
        "    assert len(X) == len(Y), \"Lengths of arrays need to match!\"\n",
        "    assert isinstance(sizes[0], float), \"Provide float arguments please!\"\n",
        "    assert isinstance(sizes[1], float), \"Provide float arguments please!\"\n",
        "    sizes = np.array(sizes)\n",
        "    sizes /= sizes.sum()\n",
        "    n = len(X)\n",
        "    ix = np.random.permutation(n)\n",
        "    intervals = np.insert(sizes, 0, 0)\n",
        "    breaks = (n * np.cumsum(intervals)).astype(int)\n",
        "    sub_ix = [ix[breaks[i]:breaks[i + 1]] for i in range(len(sizes))]\n",
        "    sub_arrs_X = [X[k] for k in sub_ix]\n",
        "    sub_arrs_Y = [Y[k] for k in sub_ix]\n",
        "    return sub_arrs_X, sub_arrs_Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "As9ApmYwXzzI",
      "metadata": {
        "id": "As9ApmYwXzzI"
      },
      "source": [
        "Let's split the dataset randomly into two non-overlapping datasets called $D_{\\text{train}}$ and $D_{\\text{test}}$ where $D_{\\text{train}}$ contains 80% of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UH1VHxFIwJuX",
      "metadata": {
        "id": "UH1VHxFIwJuX"
      },
      "source": [
        "**3.1.1** *Use the function provided to split the data to a training set $D_{\\text{train}}$, a validation set $D_{\\text{val}}$, and a test set $D_{\\text{test}}$. The sets should be sized such that $|D_{\\text{train}}| = 200$, $|D_{\\text{val}}|=50$, and $|D_{\\text{test}}| = 100$.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "\n",
        "*hint: split the test set first and then split what remains.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hP8L8oWvxE4F",
      "metadata": {
        "id": "hP8L8oWvxE4F"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "print('shape of xs_train ', xs_train.shape, '  shape of ys_train ', ys_train.shape, '\\nshape of xs_val   ', xs_val.shape, '   shape of ys_val   ', ys_val.shape, '\\nshape of xs_test  ', xs_test.shape, '  shape of ys_test  ', ys_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3_1K6S_dXcp6",
      "metadata": {
        "id": "3_1K6S_dXcp6"
      },
      "source": [
        "Here is a handy function to plot a part of the MNIST digits array according to a given label vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-2vY2sEHIEs-",
      "metadata": {
        "id": "-2vY2sEHIEs-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_digits(xs, ys, nrows=8):\n",
        "    \"\"\"\n",
        "    A function to plot the MNIST digit array sorted by a label vector\n",
        "    xs = shape (num_samples, 64)\n",
        "    ys = shape (num_samples,)\n",
        "    nrows = the number of rows you'd like to see displayed\n",
        "    \"\"\"\n",
        "    ncols = 10\n",
        "    figure(figsize = (10, 6), dpi = 80);\n",
        "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=\"all\", sharey=\"all\")\n",
        "    for j in range(ncols):\n",
        "        class_inds = np.argwhere(ys == j)\n",
        "        for i in range(nrows):\n",
        "          ax[i][j].axis('off')\n",
        "        for i in range(min(nrows, class_inds.size)):\n",
        "            x = xs[class_inds[i]]\n",
        "            if x.size == 64:\n",
        "                x = x.reshape((8,8))\n",
        "                ax[i][j].imshow(x, cmap=plt.cm.gray)\n",
        "                # ax[i][j].axis('off')\n",
        "                if i == 0:\n",
        "                  ax[i][j].set_title(j)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6J8dE_oMZNpn",
      "metadata": {
        "id": "6J8dE_oMZNpn"
      },
      "source": [
        "Plot the digits for `xs_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CY6ur6_CURLX",
      "metadata": {
        "id": "CY6ur6_CURLX"
      },
      "outputs": [],
      "source": [
        "plot_digits(xs_train,ys_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2_1VQhbFzT",
      "metadata": {
        "id": "3c2_1VQhbFzT"
      },
      "source": [
        "**3.1.2** *Write a function to fit a polynomial kernel soft-margin SVM of degree $d$ to an $n \\times 64$ dataset such as $\\mathcal{D}_{\\text{train}}$. Use the function to fit an SVM to $\\mathcal{D}_{\\text{train}}$. We will call this hypothesis $\\hat{h}_1$, or `h_1`. You may refer to the `sklearn` [library](https://scikit-learn.org/stable/modules/classes.html) and tutorials.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M99eqWJDcAaG",
      "metadata": {
        "id": "M99eqWJDcAaG"
      },
      "outputs": [],
      "source": [
        "def fit_SVM(xs,ys,d=1,C=0.5):\n",
        "  \"\"\" Fits polynomial kernel soft-margin SVM to the supplied data\n",
        "      xs = data vector of shape (num_samples, 64)\n",
        "      ys = label vector of shape (num_samples,)\n",
        "      d = polynomial degree\n",
        "      C = cost\n",
        "      output: h (the fitted model)\n",
        "  \"\"\"\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JPLqI5Y6ggbE",
      "metadata": {
        "id": "JPLqI5Y6ggbE"
      },
      "outputs": [],
      "source": [
        "h_1 = fit_SVM(xs_train,ys_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5GcIGCPlgZ7-",
      "metadata": {
        "id": "5GcIGCPlgZ7-"
      },
      "source": [
        "**3.1.3** *Write a function to make predictions using a fitted model $\\hat{h}_1$ or on some dataset $\\mathcal{D}$ or `xs, ys`. The function should return the predictions, the average loss $L(\\hat{h}_1)$ using $\\ell_{0-1}$, and the accuracy expressed as a percentage. You may again refer to the `sklearn` library.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amzDT3Ril-sZ",
      "metadata": {
        "id": "amzDT3Ril-sZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "def h_predict(h,xs,ys):\n",
        "  \"\"\"\n",
        "  xs = data vector of shape (num_samples, 64)\n",
        "  ys = label vector of shape (num_samples,)\n",
        "  h  = the fitted model\n",
        "  output:\n",
        "  y_pred = prediction vector of shape (num_samples,)\n",
        "  l01 = the zero-one loss\n",
        "  acc = the accuracy expressed as a percentage\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  return ys_pred, l01, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fi9ltFbq31j2",
      "metadata": {
        "id": "Fi9ltFbq31j2"
      },
      "source": [
        "**3.1.4** *How well did the model do? Do you notice any mistakes in its predictions? Report the empirical risk $R_\\text{emp}(\\hat{h}_1)$. Report the accuracy. Use `plot_digits` to plot a sampling of the predictions from your trained model.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "--LhcQbi5mce",
      "metadata": {
        "id": "--LhcQbi5mce"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Empirical risk R(h_1): {:.3f}\".format(risk))\n",
        "print(\"Accuracy       h_1(xs_train): {:.3f}%\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I1ZOeDOxdLz2",
      "metadata": {
        "id": "I1ZOeDOxdLz2"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bHqQI9fsqw5i",
      "metadata": {
        "id": "bHqQI9fsqw5i"
      },
      "source": [
        "**3.1.5** *Now, apply your model $\\hat{h}_1$ on dataset $\\mathcal{D}_{\\text{test}}$. Make predictions, display the test error $L_\\text{test}(\\hat{h}_1)$  using $\\ell_{0-1}$, the accuracy expressed as a percentage, and plot samples of your predictions in a figure.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2QJuYvTyq0k6",
      "metadata": {
        "id": "2QJuYvTyq0k6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Zero-one loss h_1(xs_test): {:.3f}\".format(L))\n",
        "print(\"Accuracy      h_1(xs_test): {:.3f}%\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VFgDi1aL9mAD",
      "metadata": {
        "id": "VFgDi1aL9mAD"
      },
      "source": [
        "The results were not too bad, right? I wonder what would happen if we make our model more expressive. Let's give it a try."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i9Ut4kgS9iql",
      "metadata": {
        "id": "i9Ut4kgS9iql"
      },
      "source": [
        "**3.1.6** *Make your model more expressive by increasing the polynomial degree to $d=15$. Retrain your model on $\\mathcal{D}_{\\text{train}}$ and call it $\\hat{h}_{15}$. Then report the Empirical Risk $R_{\\text{emp}}$ and accuracy on the training set and plot samples of the predictions. Then apply $\\hat{h}_{15}$ to the test set $\\mathcal{D}_{\\text{test}}$, make predictions, display the test error $L_\\text{test}(\\hat{h}_{15})$  using $\\ell_{0-1}$, the accuracy expressed as a percentage, and plot samples of your predictions in a figure.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S0gM_Mu9_s7u",
      "metadata": {
        "id": "S0gM_Mu9_s7u"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# h_15 = ...\n",
        "print(\"Empirical risk R(h_15): {:.3f}\".format(risk))\n",
        "print(\"Accuracy       h_15(xs_train): {:.3f}%\".format(acc_train))\n",
        "\n",
        "# YOUR CODE HERE\n",
        "print(\"Zero-one loss h_15(xs_test): {:.3f}\".format(L))\n",
        "print(\"Accuracy      h_15(xs_test): {:.3f}%\".format(acc_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tEMFzlTiCCNX",
      "metadata": {
        "id": "tEMFzlTiCCNX"
      },
      "source": [
        "**3.1.7** *Explain what you observed in the previous step. Describe how these results can be explained by learning theory.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g8DSrgKYCi2U",
      "metadata": {
        "id": "g8DSrgKYCi2U"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FqbUqLgADMum",
      "metadata": {
        "id": "FqbUqLgADMum"
      },
      "source": [
        "**Ooops!** We just committed one of the worst sins in ML! You started playing around with hyperparameters and checking the results on the test set. Weren't we supposed to lock the test set in a chest and throw it into the ocean until we are done?\n",
        "\n",
        "Very naughty. But seriously, don't try things out on the test set. We need it to be an impartial estimate of generalization.\n",
        "\n",
        "Let's get our validation set $\\mathcal{D}_{\\text{val}}$ out and do things the right way."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "og72MxcETidd",
      "metadata": {
        "id": "og72MxcETidd"
      },
      "source": [
        "**3.1.8** *Use the validation set to search for a good value for hyperparameter $d$, the polynomial degree. Search in the range $d = [0,20]$. Plot the training loss $L_{train}$ and validation loss $L_{val}$as a function of $d$. Report the best value for $d$ you find. Also plot and report the accuracy.* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aih6k66UoWb",
      "metadata": {
        "id": "4aih6k66UoWb"
      },
      "outputs": [],
      "source": [
        "min_degree = 0\n",
        "max_degree = 20\n",
        "degrees = np.arange(min_degree, max_degree + 1, dtype=np.int64)\n",
        "\n",
        "L_by_degree = {\n",
        "    'train': np.zeros(shape=[max_degree - min_degree + 1]),\n",
        "    'val': np.zeros(shape=[max_degree - min_degree + 1])\n",
        "}\n",
        "min_L = 100\n",
        "best_d = 0\n",
        "best_acc = 0\n",
        "for d in degrees:\n",
        "  # YOUR CODE HERE\n",
        "  if L <= min_L:\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "print('best d = ', best_d, 'with L_val = ', min_L, ' and acc = ', best_acc)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(degrees, L_by_degree['train'], label=r\"$L_{train}$\")\n",
        "ax.plot(degrees, L_by_degree['val'], label=r\"$L_{val}$\")\n",
        "ax.set_xlabel('degrees $d$')\n",
        "ax.set_ylabel('L')\n",
        "ax.plot(best_d, min_L, 'o')\n",
        "plt.legend(loc=\"lower right\");\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# plot the accuracy here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ySENcQ1hifXf",
      "metadata": {
        "id": "ySENcQ1hifXf"
      },
      "source": [
        "Great! It seems we've found a good value for the hyperparameter $d$, the polynomial degree. What if we change the regularization weight to help constrain the expressiveness of the hypothesis?\n",
        "\n",
        "Recall that for the **SVM** model specifically, we have two objectives:\n",
        "1. Find a hyperplane (i.e. decision boundary) that maximises the number of samples that are classified correctly - this is the loss term\n",
        "2. Find a hyperplane that maximises the **thickness of the margin** between the samples from the respective classes - this is the regularization term\n",
        "\n",
        "The term $C$ in an SVM controls the relative strength of the regularization term and the loss term. **Low** values of $C$ will force the algorithm to find a **larger** margin, typically at the cost of not being able to classify all samples correctly. Conversely, **large** values of $C$ will force the algorithm to find a hyperplane that leads to a **higher** accuracy, typically at the expense of a **thinner** margin. Consequently, there is a **sweet spot** when it comes to optimising $C$. Let's do a *grid search* to find good values for $d$ and $C$ simultaneously!\n",
        "\n",
        "<br>\n",
        "\n",
        "**3.1.9** *Use the validation set to search for values for hyperparameters $d$ and $C$. Search in the range $d = [0,20]$, $C=\\{10^{-5}, 10^{-4.5}, \\ldots, 10^1\\}$. Use the code below to make a contour plot of the validation loss $L_{val}$ as a function of $d$ and $C$. Report the best values for $d$ and $C$ you find.* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wT0EhOahpv95",
      "metadata": {
        "id": "wT0EhOahpv95"
      },
      "outputs": [],
      "source": [
        "degrees = np.arange(min_degree, max_degree + 1, dtype=np.int64)\n",
        "Cs_exp = np.arange(-5, 0.5, 0.5, dtype=float)\n",
        "Cs = 10.0**Cs_exp\n",
        "L_by_C_by_degree = np.zeros(shape=[len(Cs), len(degrees)])\n",
        "\n",
        "min_L = 100 # we arbitrarily set 100 here since it is a large number\n",
        "best_d = 0\n",
        "best_C = 0\n",
        "for C_id, C in enumerate(Cs):\n",
        "  for degree_id, degree in enumerate(degrees):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "print('best d = ', best_d, 'best C = ', best_C, 'with L_val = ', min_L)\n",
        "fig, ax = plt.subplots()\n",
        "CS = ax.contour(degrees, Cs_exp, L_by_C_by_degree, 10)\n",
        "ax.clabel(CS, inline=True, fontsize=10)\n",
        "ax.set_title('Hyperparameter search for best $d$ and $C$')\n",
        "ax.set_xlabel('degrees $d$')\n",
        "ax.set_ylabel(' $log_{10} \\, C$')\n",
        "ax.axis([0, 20, -5, 1])\n",
        "ax.plot(best_d, np.log10(best_C), 'o')\n",
        "ax.annotate('minimum L', xy=(best_d, np.log10(best_C)), xytext=(8, -1),\n",
        "            arrowprops=dict(facecolor='black', shrink=0.05));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "698WRMKCqhOE",
      "metadata": {
        "id": "698WRMKCqhOE"
      },
      "source": [
        "That looks like a pretty good model. Let's lock down that model. Since we are finished developing the model, **now** we apply it on the test set to see how it performs.\n",
        "\n",
        "**3.1.10** *Apply your final model to the test set to estimate how it generalizes. Report the test loss $L_{test}$ and accuracy. Plot some samples of the clasification results. Comment on the results you see and compare it to when you peeked at the test set earlier on.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MzrQ5FpYrmQk",
      "metadata": {
        "id": "MzrQ5FpYrmQk"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Zero-one loss h_15(xs_test): {:.3f}\".format(L))\n",
        "print(\"Accuracy      h_15(xs_test): {:.3f}%\".format(acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nZ-1j3hEr2IM",
      "metadata": {
        "id": "nZ-1j3hEr2IM"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y3tRneLPv1IA",
      "metadata": {
        "id": "y3tRneLPv1IA"
      },
      "source": [
        "To wrap up this part of the exercise, I am going to shift the digits to the right in the test set a bit. We will call this new set $\\mathcal{D}_{\\text{domain_shift}}$ or `xs_ds` and `ys_ds`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6sY8N3GHstPP",
      "metadata": {
        "id": "6sY8N3GHstPP"
      },
      "outputs": [],
      "source": [
        "xs_ds = xs_test.reshape((-1,8,8))\n",
        "ys_ds = ys_test\n",
        "for i in np.arange(1,100):\n",
        "  xs_ds[i,:,:] = np.roll(xs_ds[i,:,:], 1)\n",
        "xs_ds = np.reshape(xs_ds, newshape=[-1, 8 * 8])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HU2GnZYXwQNx",
      "metadata": {
        "id": "HU2GnZYXwQNx"
      },
      "source": [
        "**3.1.11** *Apply your final model to $\\mathcal{D}_{\\text{domain_shift}}$ to estimate how it generalizes there. Report the test loss $L_{test}$ and accuracy. Plot some samples of the clasification results `plot_digits`. Comment on the results you see and compare it to your previous results. Explain why the results appear as they do.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E1VP1JvbwsFY",
      "metadata": {
        "id": "E1VP1JvbwsFY"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9oezWLIz6zw",
      "metadata": {
        "id": "b9oezWLIz6zw"
      },
      "source": [
        "# 3.2 Bayes Optimial Hypothesis (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I6ELTP8K7bHP",
      "metadata": {
        "id": "I6ELTP8K7bHP"
      },
      "source": [
        "In the Lecture Notes, we relied on an expression for the Bayes optimal hypothesis, $h_{bayes}^\\text{square}$, in the bias-variance decomposition of the $\\ell_{square}$ loss.\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "h_{bayes}^\\text{square} (x) &= \\arg \\min_{h} \\mathbb{E}_{ \\mathcal{P}_{\\pmb{y}|\\pmb{x}}} [ \\, \\ell_{square}   (h(x), \\pmb{y})  \\,] \\\\\n",
        "h_{bayes}^\\text{square} (x) &= \\arg \\min_{h} \\mathbb{E}_{ \\mathcal{P}_{\\pmb{y}|\\pmb{x}}} [ (h(x) - \\pmb{y} )^2 ] \\\\\n",
        " &= \\arg \\min_{\\hat{{y}}} \\mathbb{E}_{ \\mathcal{P}_{\\pmb{y}|\\pmb{x}}} [ (\\hat{y} - \\pmb{y} )^2 ]\n",
        "\\end{align}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CsaIuSNZ0KoX",
      "metadata": {
        "id": "CsaIuSNZ0KoX"
      },
      "source": [
        "**3.2.1** *Starting from the expression above, show that*\n",
        "\n",
        "$$\n",
        "h_{bayes}^\\text{square} = \\mathbb{E}_{\\mathcal{P}_{y|\\pmb{x}}} [\\pmb{y}]\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "*Hint: rewrite using the definition of the expectation first, then take derivative w.r.t.* $\\hat{y}$. $\\color{red}{\\text{(3 points)}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YKr4L4SJ1GbP",
      "metadata": {
        "id": "YKr4L4SJ1GbP"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7095c7",
      "metadata": {
        "id": "af7095c7"
      },
      "source": [
        "# 3.3 Generalization using Cross Validation (5 points + 2 bonus points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i8khlX8l5xs_",
      "metadata": {
        "id": "i8khlX8l5xs_"
      },
      "source": [
        "There are a number of shortcomings of estimating the test error a single time as we did above in 3.1. For example, if the dataset is small, we would like to use the entire dataset to estimate the test error (as well as to train the model). Also, if we are interested in statistical properties of the test error, *e.g.* the mean, standard deviation, or confidence interval, we lack the means to obtain these statistics. We might want to calculate these statistics in order to show that one model is superior over another.\n",
        "\n",
        "A (partial) solution to this dilemma is called **cross validation**. The basic idea is to divide the data into equal partitions and iterate over the partitions, each time using one partition as the test set and the others for training. Suppose that our dataset is given by $D = \\lbrace (x_i, y_i)\\rbrace_{i=1}^n$. A vanilla version of cross validation is $K$-fold cross validation in which we do the following:\n",
        "1. Pick $K$ (e.g. $K=5$)\n",
        "2. Shuffle $D$\n",
        "3. Divide $D$ into $K$ non-overlapping subsets $D_1, \\ldots, D_K$ by letting $D_1$ have the first $\\frac{n}{K}$ indices of $D$, $D_2$ the next $\\frac{n}{K}$ of $D$ etc.\n",
        "4. For each $k = 1, \\ldots, K$, train the model on the **union** of all $D_i$ for $i \\neq k$ and report it's quality on $D_k$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wQ9xWl1q8FWn",
      "metadata": {
        "id": "wQ9xWl1q8FWn"
      },
      "source": [
        "Let's load digit images from MNIST again. This time we will use $1,000$ examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKk71Vhg8WTP",
      "metadata": {
        "id": "UKk71Vhg8WTP"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "data = load_digits()\n",
        "imgs = data['images'] # shape (1797, 8, 8)\n",
        "targets = data['target'] # shape (1797,)\n",
        "\n",
        "n = 1000\n",
        "xall = np.reshape(imgs, newshape=[-1, 8 * 8]) # shape (1797, 64)\n",
        "yall = targets # shape (1797,)\n",
        "\n",
        "ix = np.arange(1000)\n",
        "xs = xall[ix] # shape (n, 64)\n",
        "ys = yall[ix] # shape (n,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f7c881",
      "metadata": {
        "id": "21f7c881"
      },
      "source": [
        "**3.3.1.** *Create a function ```generate_folds``` that returns $K$ sets of non-overlapping indices. To make the sets have equal length, you should discard the remainder of the data.*  $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J0MyOWox9GhI",
      "metadata": {
        "id": "J0MyOWox9GhI"
      },
      "outputs": [],
      "source": [
        "def generate_folds(ys: np.ndarray, K: int):\n",
        "    \"\"\"\n",
        "    Generates 'K' folds (indices). If 'N' is not divisible by 'K' the remainder of the data is discarded.\n",
        "    Input:\n",
        "        ys: An np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        ix: An np.ndarray of shape [K, N // K] containing the indices of each fold\n",
        "    \"\"\"\n",
        "   assert K > 0, \"K needs to be positive!\"\n",
        "\n",
        "   # YOUR CODE HERE\n",
        "\n",
        "   return ix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380e1a9e",
      "metadata": {
        "id": "380e1a9e"
      },
      "source": [
        "**3.3.2.** *Write a loop over $k=1, \\ldots, K$ for $K=5$ that performs $K$-fold cross validation. Use SK-Learn's implementation of a soft-margin SVM with an* RBF Kernel, *and otherwise default parameters. Compute and report the accuracy for each fold $k$ along with the mean accuracy and the standard deviation.*  $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "srmnoQHK_-m6",
      "metadata": {
        "id": "srmnoQHK_-m6"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "K = 5\n",
        "accuracy_on_fold = np.zeros(shape=[K])\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(f\"{accuracy_on_fold.mean()} {K}-fold CV accuracy\")\n",
        "print(f\"{accuracy_on_fold.std()} {K}-fold CV accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c87bab51",
      "metadata": {
        "id": "c87bab51"
      },
      "source": [
        "Using our new knowledge on cross validation, let's now find and optimal value of $C$ for our RBF kernel SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f3e2e31",
      "metadata": {
        "id": "6f3e2e31"
      },
      "source": [
        "**3.3.3.** Perform $K$-fold cross validation for $K=5$, and in each fold loop over $C=0.1, 0.2, \\ldots, 5.0$. Thus, you will have nested loops for $k=1, \\ldots, K$ for $K=5$ (outer) and $C=0.1, 0.2, \\ldots, 5.0$ (inner).\n",
        "For each fold $k$, use SK-Learn's **SVM** implementation and compute the `accuracy_per_fold` for each $C$. Then, compute the mean and standard deviations of `accuracy_per_fold` over the folds. Use the code provided below to print the best mean accuracy and corresponding standard deviation, along with a plot of all accuracies and standard deviations.*  $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uNfkxfcFLtID",
      "metadata": {
        "id": "uNfkxfcFLtID"
      },
      "outputs": [],
      "source": [
        "K = 5\n",
        "Cs = np.arange(0.1, 5.0, 0.1)\n",
        "accuracy_per_fold = np.zeros(shape=[len(Cs), K])\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "mean_accuracy = np.mean(accuracy_per_fold, axis=1)\n",
        "stdev = np.std(accuracy_per_fold, axis=1)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "# provided printing and plotting\n",
        "assert mean_accuracy.shape == (49,)\n",
        "assert stdev.shape == (49,)\n",
        "print(f\"{best_acc.item():.3f} best mean {K}-fold CV accuracy at C={best_C.item():.3f}\")\n",
        "print(f\"{best_stdev.item():.3f} {K}-fold CV standard deviation at C={best_C.item():.3f}\")\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(Cs, mean_accuracy, 'k-')\n",
        "ax.fill_between(Cs, mean_accuracy-stdev, mean_accuracy+stdev,\n",
        "    alpha=0.2, edgecolor='#1B2ACC', facecolor='#089FFF',\n",
        "    linewidth=0, linestyle='dashdot', antialiased=True);\n",
        "ax.plot(best_C, best_acc, 'o')\n",
        "ax.annotate('best C', xy=(best_C, best_acc), xytext=(2.5, 95), arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "ax.set_xlabel('$C$')\n",
        "ax.set_ylabel('mean_accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R_FzU_bcNO8D",
      "metadata": {
        "id": "R_FzU_bcNO8D"
      },
      "source": [
        "**Oh no!** You didn't do it again, did you? You weren't using test splits to choose $C$, were you? I hope those were validation folds! I'll give you the benefit of the doubt and assume they were validation folds.\n",
        "\n",
        "<br>\n",
        "\n",
        "If you want to use cross validation to both do model selection *and* to assess generalization, you need to use **nested cross validation** which nests a validation loop inside each test loop. We won't do that here, because luckily, there we still have some data samples we never used (we used $1,000$ of the $1,797$ in total), and this can serve as our test set $\\mathcal{D}_{test}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YqY0-r27ODUZ",
      "metadata": {
        "id": "YqY0-r27ODUZ"
      },
      "outputs": [],
      "source": [
        "ix = np.arange(1000,1797)\n",
        "xs_test = xall[ix]\n",
        "ys_test = yall[ix]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xiqftWZFPCcp",
      "metadata": {
        "id": "xiqftWZFPCcp"
      },
      "source": [
        "Now, apply the best model you found on the test set to estimate the generalizaiton accuracy.\n",
        "\n",
        "**3.3.4.** *Apply the best model you found in 3.3.3 on the test set and report the accuracy.*  $\\color{red}{\\text{(1 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fsLMxOqeRZIm",
      "metadata": {
        "id": "fsLMxOqeRZIm"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "print(f\" {accuracy:.3f} accuracy on test set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbafd9fe",
      "metadata": {
        "id": "fbafd9fe"
      },
      "source": [
        "In practice, it is often beneficial to use a **stratified** variant of cross validation. Suppose that we have $K$ classes and each class $i$ occurs with frequency $\\omega_i = \\frac{n_i}{n}$ where $n_i = |\\lbrace j | y_j = i \\rbrace|$ is the count of examples in class $i$. Building the folds as we did before, it can happen that some fold $D_k$ doesn't feature the same class frequencies as the whole dataset $D$. This, in turn, can lead to poor estimations.\n",
        "\n",
        "**Stratification** means that we attempt to sample the folds in such a way that the class frequencies are roughly the same as they are on the whole dataset. Let's plot the **class frequencies** of the digits dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa9aed3",
      "metadata": {
        "id": "afa9aed3"
      },
      "outputs": [],
      "source": [
        "class_labels, class_counts = np.unique(ys, return_counts=True)\n",
        "class_freqs = class_counts/n\n",
        "sns.barplot(x=class_labels, y=class_freqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4035eb9b",
      "metadata": {
        "id": "4035eb9b"
      },
      "source": [
        "Let's repeat what we did before. For each fold we pick $\\frac{n}{K}$ samples uniformly at random from the dataset without replacement. How even are the distributions across the folds?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M90IIQKAUPrI",
      "metadata": {
        "id": "M90IIQKAUPrI"
      },
      "outputs": [],
      "source": [
        "K = 5\n",
        "\n",
        "# shuffle the data\n",
        "ix = np.random.choice(len(xs), size=n, replace=False)\n",
        "xs = xs[ix] # shape (n, 64)\n",
        "ys = ys[ix] # shape (n,)\n",
        "\n",
        "# generate fold indexes\n",
        "folds_ix = generate_folds(ys, K=K)\n",
        "\n",
        "# plot the frequencies\n",
        "fig, ax = plt.subplots(nrows=K, ncols=1, sharex=True, sharey=True)\n",
        "for i, fold_ix in enumerate(folds_ix):\n",
        "    class_labels_fold, class_counts_fold = np.unique(ys[fold_ix], return_counts=True)\n",
        "    class_freqs_fold = class_counts_fold/class_counts_fold.sum()\n",
        "    sns.barplot(x=class_labels_fold, y=class_freqs_fold, ax=ax[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbc2876",
      "metadata": {
        "id": "ccbc2876"
      },
      "source": [
        "It looks like some of the folds are a bit unbalanced, huh? A solution would ensure that each fold has the same distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b148a5",
      "metadata": {
        "id": "e4b148a5"
      },
      "source": [
        "**3.3.5.** *Write the function ```generate_folds_by_stratification``` that generates folds with stratified sampling.*  $\\color{blue}{\\text{(2 bonus points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BVrkmBhzV75X",
      "metadata": {
        "id": "BVrkmBhzV75X"
      },
      "outputs": [],
      "source": [
        "def generate_folds_by_stratification(ys: np.ndarray, K: int):\n",
        "    \"\"\"\n",
        "    Generates 'K' folds (indices) by stratification. Each fold is guaranteed to have the same label distribution.\n",
        "    If 'n' is not divisible by 'K' the remainder of the data is discarded.\n",
        "    Input:\n",
        "        ys: An np.ndarray of shape [n]\n",
        "    Returns:\n",
        "        An np.ndarray of shape [K, n // K] containing the indices of each fold\n",
        "    \"\"\"\n",
        "\n",
        "    assert K > 0, \"K needs to be positive!\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return np.asarray(ix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a726962b",
      "metadata": {
        "id": "a726962b"
      },
      "source": [
        "Let us visualize the label distribution of the individual folds using stratified sampling with `generate_folds_by_stratification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c107e0b",
      "metadata": {
        "id": "8c107e0b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "K = 5\n",
        "folds_ix = generate_folds_by_stratification(ys, K=K)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=K, ncols=1, sharex=True, sharey=True)\n",
        "for i, fold_ix in enumerate(folds_ix):\n",
        "    class_labels_fold, class_counts_fold = np.unique(ys[fold_ix], return_counts=True)\n",
        "    class_freqs_fold = class_counts_fold/class_counts_fold.sum()\n",
        "    sns.barplot(x=class_labels_fold, y=class_freqs_fold, ax=ax[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K9wA16hXvGQE",
      "metadata": {
        "id": "K9wA16hXvGQE"
      },
      "source": [
        "# 3.4 Performance metrics (2 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6PhqNqELj5by",
      "metadata": {
        "id": "6PhqNqELj5by"
      },
      "source": [
        "In this section we are going to ask you to implement some of the most important performance metrics in ML. In order to make things as simple as possible, we will first convert our earlier multi-class problem to a binary one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_BQ75Q7u08cD",
      "metadata": {
        "id": "_BQ75Q7u08cD"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from matplotlib import pyplot as plt\n",
        "data = load_digits()\n",
        "imgs = data['images'] # shape (1797, 8, 8)\n",
        "targets = data['target'] # shape (1797,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Smymu0doUOQz",
      "metadata": {
        "id": "Smymu0doUOQz"
      },
      "source": [
        "Here we just randomly decided to classify only the digits 8 and 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rJMiJjN21BTy",
      "metadata": {
        "id": "rJMiJjN21BTy"
      },
      "outputs": [],
      "source": [
        "binary_samples = np.logical_or(targets==8, targets==9)\n",
        "imgs = imgs[binary_samples]\n",
        "targets = targets[binary_samples]-8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nM70yWp7AEcX",
      "metadata": {
        "id": "nM70yWp7AEcX"
      },
      "source": [
        "Next, we split the data into two partitions. We will train a model with preset hyperparameters later so we do not need a validation set in this instance. Consider what we are doing here for a moment. Is it ok to not have a validation set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aYH4-q_108cE",
      "metadata": {
        "id": "aYH4-q_108cE"
      },
      "outputs": [],
      "source": [
        "num_samples = 350\n",
        "xs = np.reshape(imgs, newshape=[-1, 8 * 8]) # shape (1797, 64)\n",
        "ys = targets # shape (1797,)\n",
        "ix = np.random.choice(len(xs), size=num_samples, replace=False)\n",
        "xs = xs[ix] # shape (num_samples, 64)\n",
        "ys = ys[ix] # shape (num_samples,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Juyyv-25a1O",
      "metadata": {
        "id": "-Juyyv-25a1O"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "X, Y = split_into_subsets(X=xs, Y=ys, sizes=(100.0, 250.0))\n",
        "xs_test = X[0]\n",
        "ys_test = Y[0]\n",
        "xs_train = X[1]\n",
        "ys_train = Y[1]\n",
        "print('shape of xs_train ', xs_train.shape, '  shape of ys_train ', ys_train.shape, '\\nshape of xs_test  ', xs_test.shape, '  shape of ys_test  ', ys_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W_gI41ztAfWB",
      "metadata": {
        "id": "W_gI41ztAfWB"
      },
      "source": [
        "We also apply a slight domain shift as we did previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3U0ZK91yf2qU",
      "metadata": {
        "id": "3U0ZK91yf2qU"
      },
      "outputs": [],
      "source": [
        "xs_ds = xs_test.reshape((-1,8,8))\n",
        "ys_ds = ys_test\n",
        "for i in np.arange(1,100):\n",
        "  xs_ds[i,:,:] = np.roll(xs_ds[i,:,:], 1)\n",
        "xs_ds = np.reshape(xs_ds, newshape=[-1, 8 * 8])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PoK1TP5ZAlqg",
      "metadata": {
        "id": "PoK1TP5ZAlqg"
      },
      "source": [
        "Finally we train a logistic regression classifier with some fixed parameters. You can also try to train your own implementation from Exercise 2 here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X138C_G6U1NG",
      "metadata": {
        "id": "X138C_G6U1NG"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0, max_iter=100, tol=1e-2).fit(xs_train, ys_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56TeOUkRA3Uo",
      "metadata": {
        "id": "56TeOUkRA3Uo"
      },
      "source": [
        "Here is a way to report the accuracy of this set of predictions. Later, we are going to ask you to implement a similar function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1EDhbJmxA0uo",
      "metadata": {
        "id": "1EDhbJmxA0uo"
      },
      "outputs": [],
      "source": [
        "accuracy = clf.score(xs_test, ys_test)\n",
        "print(f\"Accuracy of this binary classifier is {accuracy*100}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edzXg8nKBAG5",
      "metadata": {
        "id": "edzXg8nKBAG5"
      },
      "source": [
        "Similarly, we compute the precision, recall and confusion matrix for this set of predictions using ```sklearn``` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-zwjltKjjWQ",
      "metadata": {
        "id": "H-zwjltKjjWQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oc6VWQyllmAS",
      "metadata": {
        "id": "Oc6VWQyllmAS"
      },
      "outputs": [],
      "source": [
        "predictions = clf.predict(xs_test)\n",
        "CM = confusion_matrix(ys_test, predictions)\n",
        "TN = CM[0][0]\n",
        "FN = CM[1][0]\n",
        "TP = CM[1][1]\n",
        "FP = CM[0][1]\n",
        "print(f\"The number of true positives is {TP}, true negatives is {TN}, false positives is {FP} and false negatives is {FN}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yc5JdeyWCD61",
      "metadata": {
        "id": "Yc5JdeyWCD61"
      },
      "source": [
        "And here is one way to visualize the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gPLiyI46CBZ1",
      "metadata": {
        "id": "gPLiyI46CBZ1"
      },
      "outputs": [],
      "source": [
        "s = sns.heatmap(CM, annot=True, cbar=False)\n",
        "s.set(ylabel='Ground truth', xlabel='Predictions');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bxub7RA7kSa3",
      "metadata": {
        "id": "bxub7RA7kSa3"
      },
      "outputs": [],
      "source": [
        "precision = precision_score(ys_test, predictions)\n",
        "recall = recall_score(ys_test, predictions)\n",
        "print(f\"The precision of the classifier is {precision} and its recall is {recall}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O1QLJG6fBQQg",
      "metadata": {
        "id": "O1QLJG6fBQQg"
      },
      "source": [
        "Now, you will get to implement these metrics for yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BjXkuOnkmFjj",
      "metadata": {
        "id": "BjXkuOnkmFjj"
      },
      "source": [
        "### Accuracy\n",
        "**3.4.1.** *Write the function ```get_accuracy``` that calculates the accuracy of a prediction given the ground truth. In previous steps we had imported ```accuracy_score``` function from sklearn to do this, now you are implementing that for yourself.*  $\\color{red}{\\text{(0.5 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94yurvdukBNI",
      "metadata": {
        "id": "94yurvdukBNI"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(y_true, y_pred):\n",
        "  # YOUR CODE HERE\n",
        "  return acc\n",
        "predictions = clf.predict(xs_test)\n",
        "print(f\"Accuracy of the prediction is {get_accuracy(ys_test, predictions)*100}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y9NHR7jysdVS",
      "metadata": {
        "id": "Y9NHR7jysdVS"
      },
      "source": [
        "### Confusion matrix\n",
        "**3.4.2.** *Write the function ```get_cm``` that computes the TN, FN, TP, FP counts and returns them as a [binary confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). We have already seen a confusion matrix in the lecture notes. What the axes represent is just a convention and here we opt for sklearn's convention like we did above.*  $\\color{red}{\\text{(1 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBvNPnHkrn0l",
      "metadata": {
        "id": "pBvNPnHkrn0l"
      },
      "outputs": [],
      "source": [
        "def get_cm(y_true, y_pred):\n",
        "  # YOUR CODE HERE\n",
        "  return cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xNt0Wz0U7zhJ",
      "metadata": {
        "id": "xNt0Wz0U7zhJ"
      },
      "source": [
        "And next you can run and visualize your solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z5i96E507tLj",
      "metadata": {
        "id": "z5i96E507tLj"
      },
      "outputs": [],
      "source": [
        "cm = get_cm(ys_test, predictions)\n",
        "assert np.all(cm == CM)\n",
        "s = sns.heatmap(cm, annot=True, cbar=False)\n",
        "s.set(ylabel='Ground truth', xlabel='Predictions');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QPN0yRk4ptsO",
      "metadata": {
        "id": "QPN0yRk4ptsO"
      },
      "source": [
        "### Precision and recall\n",
        "_Precision_ measures the fraction of positive classifications that are correct (that actually are true positives). Precision is useful when the cost of false positives is high, e.g. you have built a medical screening test for local *vårdcentral*s and each false positive means someone will have to go through an expensive and painful biopsy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229f69f5",
      "metadata": {
        "id": "229f69f5"
      },
      "source": [
        "The recall measures the fraction of the true positives that are correctly classified by our classifier. It is also called the _True Positive Rate (TPR)_ or the _sensitivity_. Recall is important when missing *true* samples is costly, e.g. you have built a diagnostic test for Karolinska Hospital and each false negative means someone might die."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qLVuHUQMu6Oq",
      "metadata": {
        "id": "qLVuHUQMu6Oq"
      },
      "source": [
        "**Task 3.4.3:** *Write the function ```get_precision_recall``` that computes the precision and recall given the predictions and the true labels. You can and should use your ```get_cm``` function here.* $\\color{red}{\\text{(0.5 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2HAnKeMvA4nL",
      "metadata": {
        "id": "2HAnKeMvA4nL"
      },
      "outputs": [],
      "source": [
        "def get_precision_recall(y_true, y_pred):\n",
        "    # YOUR CODE HERE\n",
        "    return precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T9IcwK659iIb",
      "metadata": {
        "id": "T9IcwK659iIb"
      },
      "outputs": [],
      "source": [
        "precision, recall = get_precision_recall(ys_test, predictions)\n",
        "assert precision_score(ys_test, predictions) == precision\n",
        "assert recall_score(ys_test, predictions) == recall\n",
        "print(f\"The precision of the classifier is {precision} and its recall is {recall}.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "144080e8"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
