{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ecce179f",
      "metadata": {
        "id": "ecce179f"
      },
      "outputs": [],
      "source": [
        "#! Required python packages: numpy, scipy, scikit-learn, matplotlib, seaborn\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hlP47NhhLUFw",
      "metadata": {
        "id": "hlP47NhhLUFw"
      },
      "source": [
        "# Exercise 2 - ML & Optimization  (DD1420 HT23)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This Jupyter Notebook contains the exercises for DD1420 **Module 2 ML & Optimization**. There are a several problems in this exercise, each broken into numbered subproblems, *e.g.* 1.1.1. The points for each problem are marked *e.g.* $\\color{red}{\\text{(} x \\text{ points)}}$. In total, the exercise is worth **31** points.\n",
        "\n",
        "<br>\n",
        "\n",
        "Students are allowed to work on this exercise in pairs. Make sure you have formed a group in Canvas with your partner. Each student is responsible for following the Code of Conduct. In particular (1) All members of a group are responsible for the group's work, (2) Every student shall honestly disclose any help received and sources used, and (3) Do not copy from other people's solutions. If you need assistance with the exercise, you are encouraged to post a question to the appropriate Discussion Topic or sign up for a help session.\n",
        "\n",
        "<br>\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and delete `raise NotImplementedError()` once you have implemented the solution.\n",
        "\n",
        "<br>\n",
        "\n",
        "You should not import any libraries on top of the ones included in the assignment. Derivation questions can be answered using $\\LaTeX$, or you may upload an image of your derivation. To do so in *Google Colab* simply create a text cell, click on the `insert image` icon, and upload an image to the notebook as we have demonstrated below.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Submission** - once you have completed the exercise make sure everything runs as expected by going into `Runtime` -> `Restart and Run all`. Then download the notebook by clicking `file` -> `download` -> `download .ipynb`. Then **rename the file to include your name** (and **your partner's name** if you have one) as follows\n",
        "\n",
        "<br>\n",
        "\n",
        "`Ex??_LASTNAME_FIRSTNAME_and_LASTNAME_FIRSTNAME.ipynb`\n",
        "\n",
        "<br>\n",
        "\n",
        "where you replace `??` with the correct exercise number. If you are working alone you do not need to include a partner name.\n",
        "\n",
        "<br>\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wLs7mE3KolE7",
      "metadata": {
        "id": "wLs7mE3KolE7"
      },
      "source": [
        "![h.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z9gy3oaCdXtf",
      "metadata": {
        "id": "Z9gy3oaCdXtf"
      },
      "source": [
        "#Name\n",
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly):\n",
        "\n",
        "<br>\n",
        "\n",
        "### Student 1\n",
        "\n",
        "### Student 2  (if you work with a partner)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adefc2aa",
      "metadata": {
        "id": "adefc2aa"
      },
      "source": [
        "## 2.1 Linear Regression with RANSAC (3 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5-k1Jvv4iMf_",
      "metadata": {
        "id": "5-k1Jvv4iMf_"
      },
      "source": [
        "A mad scientist has created an artificial dataset $\\mathcal{D}=\\lbrace (x_i, y_i) \\rbrace_{i=1}^n$ according to the definition $y = e \\cdot x + e + \\varepsilon$ where $e$ is the *Euler constant* and where $\\varepsilon \\in U(-1, 1)$, i.e. $\\varepsilon$ is an error sampled from the uniform distribution defined on the interval $(-1, +1)$.\n",
        "\n",
        "Suppose we are interested in figuring out what the scientist was up to. Our goal is to retrieve the parameters $w=e$ and $b=e$ by performing a linear regression on the dataset. The dataset $\\mathcal{D}$ is generated as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c3d211ef",
      "metadata": {
        "id": "c3d211ef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "N = 100\n",
        "xs = np.linspace(0, 10, N)\n",
        "ys = np.e * xs + np.e + (2 * np.random.rand(N) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b052174",
      "metadata": {
        "id": "7b052174"
      },
      "source": [
        "Observe that ```np.random.rand``` generates uniform samples in the unit interval $(0, 1)$. We can transform them into our desired $(-1, 1)$ interval by multiplying each sample by 2 and subtracting 1 from it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c266d9",
      "metadata": {
        "id": "17c266d9"
      },
      "source": [
        "We can plot the points as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1597269e",
      "metadata": {
        "id": "1597269e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x1fb4fc5d1e0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(xs, ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81d9190",
      "metadata": {
        "id": "c81d9190"
      },
      "source": [
        "We will apply the **RANSAC** algorithm to find  $w$ and $b$. The algorithm requires three steps:\n",
        "1. Choosing two points from the dataset\n",
        "2. Finding the line that crosses both points\n",
        "3. Checking how well the found line matches the other points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9a69f3",
      "metadata": {
        "id": "8d9a69f3"
      },
      "source": [
        "**2.1.1.** *Write a function ```size_two_subsets = all_size_two_subsets(ls)``` that accepts a list ```ls``` as input and outputs a list ```size_two_subsets``` of all subsets of size 2.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1sU0pdg4brq",
      "metadata": {
        "id": "f1sU0pdg4brq"
      },
      "outputs": [],
      "source": [
        "def all_size_two_subsets(ls):\n",
        "  \"\"\"Returns a list of all subsets of size two.\"\"\"\n",
        "    # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69a31c0d",
      "metadata": {
        "id": "69a31c0d"
      },
      "source": [
        "**2.1.2.** *Write a function ```w, b = linear_fit(x1, y1, x2, y2)``` that accepts two points $p_1 = (x_1, y_1)$ and $p_2 = (x_2, y_2)$ and outputs the parameters $w, b $ of the line $w \\cdot x + b$ that passes through these two points.* $\\color{red}{\\text{(0.5 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yAx4vGPf41hw",
      "metadata": {
        "id": "yAx4vGPf41hw"
      },
      "outputs": [],
      "source": [
        "def linear_fit(x1, y1, x2, y2):\n",
        "  \"\"\"Returns w,b that fit (x1,y1) and (x2,y2)\"\"\"\n",
        "  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IW98MiUTq6g4",
      "metadata": {
        "id": "IW98MiUTq6g4"
      },
      "source": [
        "**2.1.3.** *Write a function ```score_candidate(w, b, xs, ys)``` that computes how well the found linear equation fits the dataset. For this, calculate the difference between $y_i$ and $w \\cdot x_i + b$ for each $x_i$. Return the RMSE.* $\\color{red}{\\text{(0.5 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pa6Ejft945F5",
      "metadata": {
        "id": "pa6Ejft945F5"
      },
      "outputs": [],
      "source": [
        "def score_candidate(w, b, xs, ys):\n",
        "  \"\"\"Returns the RMSE for a given w,b\"\"\"\n",
        "  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_360dNTHpmT3",
      "metadata": {
        "id": "_360dNTHpmT3"
      },
      "source": [
        "How good of a fit we get if we use the first two points in the list?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9egblqYuptA-",
      "metadata": {
        "id": "9egblqYuptA-"
      },
      "outputs": [],
      "source": [
        "x1 = xs[0]\n",
        "x2 = xs[1]\n",
        "y1 = ys[0]\n",
        "y2 = ys[1]\n",
        "w,b = linear_fit(x1,y1,x2,y2)\n",
        "print('RMSE =',score_candidate(w,b,xs,ys))\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, w * xs + b, 'y-', linewidth=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "It57MMSDrCKT",
      "metadata": {
        "id": "It57MMSDrCKT"
      },
      "source": [
        "Let's see if we can improve this using a simplified version of RANSAC.\n",
        "\n",
        "<br>\n",
        "\n",
        "**2.1.4.** * Evaluate ```score_candidate``` for every two-element subset from ```size_two_subsets``` and output the best parameters $w^\\star, b^\\star$. Print the RMSE along with $w$ and $b$ each time it improves.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PZ3ZWgUt5BCn",
      "metadata": {
        "id": "PZ3ZWgUt5BCn"
      },
      "outputs": [],
      "source": [
        "w_star, b_star = None, None\n",
        "\n",
        "pts = np.stack([xs, ys], axis=1)\n",
        "size_two_subsets = all_size_two_subsets(pts)\n",
        "w_star, b_star = None, None\n",
        "best_score = np.inf\n",
        "for size_two_subset in size_two_subsets:\n",
        "  # YOUR CODE HERE\n",
        "  if current_score < best_score:\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "# plotting your solution\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, w_star * xs + b_star, 'y-', linewidth=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7649a937",
      "metadata": {
        "id": "7649a937"
      },
      "source": [
        "## 2.2 Applying the algebraic solution to linear regression (3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a356cb44",
      "metadata": {
        "id": "a356cb44"
      },
      "source": [
        "Now that we have an intuition of how linear regression works, let's now repeat **2.1** with some basic linear algebra. Here is a small dataset from the mad scientist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eJg13IS6_cNo",
      "metadata": {
        "id": "eJg13IS6_cNo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n = 20\n",
        "xs = np.linspace(0, 1, n)\n",
        "ys = np.e * xs + np.pi + np.random.randn(n)\n",
        "plt.scatter(xs, ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lpi_lQ09sC-1",
      "metadata": {
        "id": "lpi_lQ09sC-1"
      },
      "source": [
        "**2.2.1** *Show that prepending $1$s to $x$ and having a single matrix multiplication using $\\theta^T$ is equivalent to $ y = w^T x + b$.*  $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "*hint: You can explicitly do the multiplication after defining the new terms, e.g.,*\n",
        "\n",
        "$ \\theta^T =\n",
        " \\begin{bmatrix}\n",
        "  b_{1} & w_{1,1} & \\cdots & w_{1,n}\\\\\n",
        "  b_{2} & w_{2,1} & \\cdots & w_{2,n} \\\\\n",
        "  \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
        "  b_{m} & w_{m,1} & \\cdots & w_{m,n}\n",
        " \\end{bmatrix} = \\begin{bmatrix}\n",
        "  b_{1} & 0 & \\cdots & 0\\\\\n",
        "  b_{2} & 0 & \\cdots & 0 \\\\\n",
        "  \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
        "  b_{m} & 0 & \\cdots & 0\n",
        " \\end{bmatrix} + \\begin{bmatrix}\n",
        "  0 & w_{1,1} & \\cdots & w_{1,n}\\\\\n",
        "  0 & w_{2,1} & \\cdots & w_{2,n} \\\\\n",
        "  \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
        "  0 & w_{m,1} & \\cdots & w_{m,n}\n",
        " \\end{bmatrix}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unAgSCbWsSK9",
      "metadata": {
        "id": "unAgSCbWsSK9"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O3z_h0TjLnX9",
      "metadata": {
        "id": "O3z_h0TjLnX9"
      },
      "source": [
        "**2.2.2** *Write a function ```b, w = linear_regression(x, y)``` that performs linear regression. Recall that the function that we want to minimize is*\n",
        "\n",
        "$L = \\frac{1}{n}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2$\n",
        "\n",
        "\n",
        " *Use the matrix solution we found in the lecture notes for $\\theta^*$. Apply it to the dataset above. Plot the solution and print the values.* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iRaZZR4sTVQ7",
      "metadata": {
        "id": "iRaZZR4sTVQ7"
      },
      "outputs": [],
      "source": [
        "def linear_regression(xs, ys):\n",
        "    \"\"\"\n",
        "    Returns an algebraic solution for w,b given\n",
        "    xs: np.ndarray of shape [n]\n",
        "    ys: np.ndarray of shape [n]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # X = ... should be of shape (20,2), remember to prepend 1 to each point\n",
        "    # X would look something like this\n",
        "    # [ 1   x_1\n",
        "    #   1   x_2\n",
        "    #   ...\n",
        "    #   1   x_n ]\n",
        "    # Y = ... should be of shape (20,1)\n",
        "    # implement the theta* equation in the matrix form we derived at the end of the lecture notes 3.2\n",
        "\n",
        "    return theta\n",
        "b, w = linear_regression(xs,ys)\n",
        "print('w = ', w, 'b = ', b)\n",
        "zs = xs*w + b\n",
        "_, ax = plt.subplots()\n",
        "ax.scatter(xs, ys)\n",
        "ax.plot(xs, zs, 'y-', linewidth=3);\n",
        "ax.legend(['Fitted line']);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978ebded",
      "metadata": {
        "id": "978ebded"
      },
      "source": [
        "## 2.3 Using linear regression for non-linear relationships (4 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aykbMiG82xWQ",
      "metadata": {
        "id": "aykbMiG82xWQ"
      },
      "source": [
        "Linear regression is often misunderstood in the sense that people believe that it can only be used to fit \"lines\" (i.e. for *linear* relationships between $x$ and $y$) even though the \"linearity\" refers to being linear with respect to the weights (i.e. the relationship between $w$ and $y$ is assumed to be linear).\n",
        "\n",
        "Take a look at graph of the dataset $\\mathcal{D} = \\lbrace (x_i, y_i) \\rbrace$ showing a non-linear relationships between $x$ and $y$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbfb1155",
      "metadata": {
        "id": "bbfb1155"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 20\n",
        "np.random.seed(0)\n",
        "xs = 2 - 3 * np.random.normal(0, 1, n)\n",
        "ys = xs - 2 * xs ** 2 + 0.5 * xs ** 3 + np.random.normal(-3, 3, n)\n",
        "plt.scatter(xs,ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b45d845",
      "metadata": {
        "id": "2b45d845"
      },
      "source": [
        "If we naively apply **SK-learn**'s linear regression function, we use the model\n",
        "\n",
        "$y = \\theta^{\\intercal}x = w^T x + b$\n",
        "\n",
        "and we just end up fitting a straight line though the points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c6f93a",
      "metadata": {
        "id": "c8c6f93a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# here is how you define and train a model with sklearn\n",
        "model = LinearRegression()\n",
        "model.fit(xs[:, np.newaxis], ys[:, np.newaxis])\n",
        "\n",
        "# here is how you make predictions\n",
        "y_pred = model.predict(xs[:, np.newaxis])\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(ys,y_pred))\n",
        "print('RMSE = ', rmse)\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, y_pred, 'y-', linewidth=3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "337c74eb",
      "metadata": {
        "id": "337c74eb"
      },
      "source": [
        "To get a better fit, we fit the data with a cubic polynomial. Define $\\Phi(x) = [1, x, x^2, x^3]^T$, then the model equation is given by:\n",
        "\n",
        "<br>\n",
        "\n",
        "$y= \\theta^T \\Phi(x) = b + w_1 x + w_2 x^2 + w_3 x^3$\n",
        "\n",
        "<br>\n",
        "\n",
        "Notice that the relationship between $$ and $y$ is still linear!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g9n9YNQCslOR",
      "metadata": {
        "id": "g9n9YNQCslOR"
      },
      "source": [
        "**2.3.1.** *Write a function ```x_poly = polynomial_features(x)```** that computes $\\Phi(x)$ for each $x$, transforming it to ``x_poly``.* $\\color{red}{\\text{(1 point)}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9wF6ffwSJIEA",
      "metadata": {
        "id": "9wF6ffwSJIEA"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "def polynomial_features(xs):\n",
        "    \"\"\"\n",
        "    x: A np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        x_poly: A np.ndarray of shape [N, 4]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return x_poly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MquajQdFnnlI",
      "metadata": {
        "id": "MquajQdFnnlI"
      },
      "source": [
        "You can do the same thing, with less coding, using the sklearn library as well. Try that here and double check you are getting the same results.\n",
        "\n",
        "*hint: [look up](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) the function we imported.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b254b65",
      "metadata": {
        "id": "5b254b65"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "def polynomial_features_sklearn(x):\n",
        "    \"\"\"\n",
        "    x: A np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        x_poly: A np.ndarray of shape [N, 4]\n",
        "    \"\"\"\n",
        "    polynomial_features= PolynomialFeatures(degree=3)\n",
        "    x_poly = polynomial_features.fit_transform(x[:, np.newaxis])\n",
        "    return x_poly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36xnGrsCm-C2",
      "metadata": {
        "id": "36xnGrsCm-C2"
      },
      "outputs": [],
      "source": [
        "assert np.allclose(polynomial_features_sklearn(xs), polynomial_features(xs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iuqMMgH2i4_9",
      "metadata": {
        "id": "iuqMMgH2i4_9"
      },
      "source": [
        "Our goal here will be to get the model to predict the outputs given the (polynomial) input features. Here is how you would get the predictions (not the coefficients) in sklearn:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(x, y)\n",
        "predictions = model.predict(x)\n",
        "```\n",
        "\n",
        "\n",
        "There is an obvious question here: Why would we try and predict *y* values for points we already know the *y*? The answer is that we usually would not, at least not in ML. This exercise is just to get your feet wet."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d280686",
      "metadata": {
        "id": "8d280686"
      },
      "source": [
        "**2.3.2** *Write a function* ```w = polynomial_linear_regression(x_poly, y)``` *that optimizes for $\\theta$ and outputs the optimal parameter vector $\\theta^* \\in \\mathbb{R}^4$.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "*hint: ```sklearn``` calls the model weights as model coefficients. as usual, you can take a look at the official documentation.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ShxKhczgJOH0",
      "metadata": {
        "id": "ShxKhczgJOH0"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def polynomial_linear_regression(x_poly, ys):\n",
        "    \"\"\"\n",
        "    x: A np.ndarray of shape [N, 4]\n",
        "    y: A np.ndarray of shape [N]\n",
        "    Returns:\n",
        "        w: A np.ndarray of shape [4]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf500f46",
      "metadata": {
        "id": "bf500f46"
      },
      "source": [
        "**2.3.3** *Write a function* ```y_poly_pred = apply_weights(x_poly, w)``` *that applies the weights to* $x$, $\\theta^{\\intercal}\\phi(x)$ *to get a prediction.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svNYDr62JVe9",
      "metadata": {
        "id": "svNYDr62JVe9"
      },
      "outputs": [],
      "source": [
        "def apply_weights(x_poly, w):\n",
        "    \"\"\"\n",
        "    x_poly: A np.ndarray of shape [N, 4]\n",
        "    w: A np.ndarray of shape [4]\n",
        "    Returns:\n",
        "      y_pred: the predicted value for x\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # return ... # do the matrix multiplication, pay attention to the dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ec6095",
      "metadata": {
        "id": "34ec6095"
      },
      "source": [
        "**2.3.4.** *Fit the model to the data to find $\\theta^*$. Compute the RMSE for the predictions your model made.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "*hint: look up the imported function, you can calculate the RMSE more conveniently with that.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "btpDvhhJKJIJ",
      "metadata": {
        "id": "btpDvhhJKJIJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "# YOUR CODE HERE\n",
        "# x_poly = ... # get the polynomial features\n",
        "# w = ... # find the fitted weights\n",
        "# y_poly_pred = ... # do the predictions\n",
        "# rmse = ...\n",
        "print('RMSE = ', rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47db12b1",
      "metadata": {
        "id": "47db12b1"
      },
      "source": [
        "And now let's visualize our beautiful polynomial fit using linear regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9233bdce",
      "metadata": {
        "id": "9233bdce"
      },
      "outputs": [],
      "source": [
        "xi = np.arange(min(xs),max(xs),0.1)\n",
        "xi_poly = polynomial_features(xi)\n",
        "yi_poly_pred = apply_weights(xi_poly,w)\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xi, yi_poly_pred,'y-')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564dccf2",
      "metadata": {
        "id": "564dccf2"
      },
      "source": [
        "## 2.4 Logistic regression  (8 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12e3cd50",
      "metadata": {
        "id": "12e3cd50"
      },
      "source": [
        "The mad scientist is at it again! He's created another strange dataset. This time, it seems to have yellow and blue samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dec033de",
      "metadata": {
        "id": "dec033de"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "xs, ys = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "plt.figure(0)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9_Hsr03vl3u",
      "metadata": {
        "id": "q9_Hsr03vl3u"
      },
      "source": [
        "This looks like a classification problem!\n",
        "\n",
        "<br>\n",
        "\n",
        "As we saw in the Lecture Notes, *logistic regression* is motivated by the desire to classify a sample (e.g. some vector) into two classes $\\pmb{y} = \\{0,1\\}$. We can model this as\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\mathcal{P}_{\\pmb{y}|\\pmb{x}} (y| \\pmb{x} = x_i) = p^{\\pmb{y}} (1-p)^{1-\\pmb{y}}\\quad $   \n",
        "\n",
        "<br>\n",
        "\n",
        "where $\\quad \\hat{p} = \\sigma(\\theta^{\\intercal} x_i) $ and $\\sigma(x) = \\frac{1}{1+e^{-x}}$ is the so-called *sigmoid* (or logistic) function. Plugging in gives\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\mathcal{P}_{\\pmb{y}|\\pmb{x}} (y| \\pmb{x} = x_i) = \\sigma(\\theta^{\\intercal} x_i)^{y_i} \\, (1 - \\sigma(\\theta^{\\intercal} x_i))^{1-y_i}$\n",
        "\n",
        "<br>\n",
        "\n",
        "and taking the $\\log$ gives the *binary cross entropy* loss\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\log \\mathcal{P}_{\\pmb{y}|\\pmb{x}} (y| \\pmb{x} = x_i)\n",
        "&= y_i \\log \\sigma(\\theta^{\\intercal} x_i) - (1 - y_i) \\log (1-\\sigma(\\theta^{\\intercal} x_i)) \\\\\n",
        "&= y_i \\log \\sigma(w^{\\intercal} x_i + b) - (1 - y_i) \\log (1-\\sigma(w^{\\intercal} x_i + b))\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "We can turn this into an average loss function $L$ by summing over the dataset and adding a minus ($-$) sign because we want to minimize it\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "L(w,b) &= -\\sum_{i=1}^n \\lbrace y_i \\cdot \\log \\sigma(w^T \\cdot x_i + b) + (1-y_i) \\cdot \\log (1 - \\sigma(w^T \\cdot x_i + b))  \\rbrace\n",
        "\\end{align}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M61KhMWjAACa",
      "metadata": {
        "id": "M61KhMWjAACa"
      },
      "source": [
        "**2.4.1** *Show that the derivative of the average binary cross entropy loss* $L$ *w.r.t. $w$ is given by (fill in the missing steps)* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zbb01sIdAz_k",
      "metadata": {
        "id": "Zbb01sIdAz_k"
      },
      "source": [
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial w}\\, L(w,b) &= \\frac{\\partial}{\\partial w}\\, -\\sum_{i=1}^n \\lbrace y_i \\cdot \\log \\sigma(w^T \\cdot x_i + b) + (1-y_i) \\cdot \\log (1 - \\sigma(w^T \\cdot x_i + b))  \\rbrace\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial w}\\, L(w,b) &= \\sum_{i=1}^n \\lbrace (1-y_i) \\cdot x_i \\cdot \\sigma(w^T x_i + b) - y_i \\cdot x_i \\cdot \\sigma(-w^T x_i - b) \\rbrace\n",
        "\\end{align}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-GZYmBzRBafW",
      "metadata": {
        "id": "-GZYmBzRBafW"
      },
      "source": [
        "Hint: a couple of identities can help simplify the equations:\n",
        "\n",
        "\\begin{align}\n",
        "1 - \\sigma(x) &= \\sigma(-x) \\\\\n",
        "\\frac{\\partial}{\\partial \\mathbf{w}} \\log \\sigma(g(\\mathbf{w})) &= \\frac{\\partial g}{\\partial \\mathbf{w}} \\cdot \\sigma(-g(\\mathbf{w}))\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i0qPhJmsG0f8",
      "metadata": {
        "id": "i0qPhJmsG0f8"
      },
      "source": [
        "**2.4.2** *Show that the derivative of the average binary cross entropy loss* $L$ *w.r.t. $b$ is given by (fill in the missing steps)* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4UgfCFD1G6Fx",
      "metadata": {
        "id": "4UgfCFD1G6Fx"
      },
      "source": [
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial b}\\, L(w,b) &= \\frac{\\partial}{\\partial b}\\, -\\sum_{i=1}^n \\lbrace y_i \\cdot \\log \\sigma(w^T x_i + b) + (1-y_i) \\cdot \\log (1 - \\sigma(w^T x_i + b))  \\rbrace\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\frac{\\partial}{\\partial b}\\, L(w,b) &= \\sum_{i=1}^n \\lbrace (1-y_i) \\cdot \\sigma(w^T x_i + b) - y_i \\cdot \\sigma(-w^T x_i - b) \\rbrace\n",
        "\\end{align}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "djzoZnINLepR",
      "metadata": {
        "id": "djzoZnINLepR"
      },
      "source": [
        "With these gradients in hand, your task is now to implement *batch stochastic gradient descent (SGD)*. Instead of looping over all samples $n$ you will loop over $B$ samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00923f3",
      "metadata": {
        "id": "d00923f3"
      },
      "source": [
        "**2.4.3**  *Write a function ```xs_batch, ys_batch = sample_batch(xs, ys, B)``` that randomly samples $B$ elements from both ```xs``` and ```ys```.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "Consider for a moment how we would write the sampling operation in mathematical notation. Most common notation would be $(x, y)  P(x,y)$, i.e. sampling from a joint distribution.\n",
        "\n",
        "But as is common, we only have a training set, not the actual distribution. How would we denote it in this case? Usually, it is denoted like $(x, y) \\sim D$ but you will come accross various different notations in the literature. In code, they are the same thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5nNwxsFMLMQJ",
      "metadata": {
        "id": "5nNwxsFMLMQJ"
      },
      "outputs": [],
      "source": [
        "def sample_batch(xs, ys, B):\n",
        "    \"\"\" Returns a batch of (xs,ys) with B elements \"\"\"\n",
        "    assert len(xs) == len(ys), \"Incompatible number of elements!\"\n",
        "    # YOUR CODE HERE\n",
        "    return xs_batch, ys_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f78456c",
      "metadata": {
        "id": "1f78456c"
      },
      "source": [
        "**2.4.4** *Write a function ```gradient_w, gradient_b = logistic_regression_gradient(xs_batch, ys_batch, w, b)``` that calculates the gradient of $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}}$ and  $\\frac{\\partial \\mathcal{L}}{\\partial b}$ with respect to the batch ```xs_batch``` (shape $[B, 2]$) and ```ys_batch``` (shape $[B]$) where $B$ is some arbitrary batch size.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "\n",
        "*hint: If you are not comfortable with vector operations, we suggest you to first solve this problem using a ```for``` loop. You can then compare your vector-based solution with the iterative ```for``` loop one to make sure you did it right.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iK5JCzm8LPkH",
      "metadata": {
        "id": "iK5JCzm8LPkH"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"Numerically-stable sigmoid function.\"\n",
        "    return np.exp(-np.logaddexp(0, -z))\n",
        "\n",
        "def logistic_regression_gradient(xs_batch, ys_batch, w, b):\n",
        "    \"\"\" Input: xs_batch [B x 2]\n",
        "               ys_batch [B x 1]\n",
        "        Returns: gradient_w, gradient_b \"\"\"\n",
        "    assert len(xs_batch) == len(ys_batch), \"Incompatible number of elements!\"\n",
        "    assert len(w) == xs_batch.shape[1], \"Incompatible shape of parameter w!\"\n",
        "    # YOUR CODE HERE\n",
        "    return gradient_w, gradient_b # take care that the returned values are calculated for a batch, not a single input/output pair"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb4873c",
      "metadata": {
        "id": "fbb4873c"
      },
      "source": [
        "**2.4.5** *Implement and apply* Stochastic Gradient Descent *to solve the logistic regression problem. Initialize the model with $w=[1 , 1]^T, b = 0$ and update the weights for $t = 10^4$ steps with a learning rate of $\\alpha = 10^{-3}$. Apply SGD on ```xs``` and ```ys``` using a batch size $B = 16$. For each step calculate and record the accuracy of the trained model.* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XMi6QDN2Ling",
      "metadata": {
        "id": "XMi6QDN2Ling"
      },
      "outputs": [],
      "source": [
        "T = 10**4\n",
        "lr = 10**(-3)\n",
        "w = np.ones(2)\n",
        "ws = np.zeros(shape=[T, 2]) # store the w vector here at each step\n",
        "b = 0.0\n",
        "B = 16\n",
        "accuracy = np.zeros(T)\n",
        "\n",
        "for t in range(T):\n",
        "  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b043f6f4",
      "metadata": {
        "id": "b043f6f4"
      },
      "source": [
        "Let's plot the solution and visualize the decision boundary together with the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa232da",
      "metadata": {
        "id": "efa232da"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w1, w2 = w\n",
        "yy = np.linspace(np.min(xs[:, 1]), np.max(xs[:, 1]), 20)\n",
        "xx = (w2 * yy + b)/(-w1)\n",
        "plt.figure(1)\n",
        "plt.plot(xx, yy, color=\"orange\")\n",
        "class_membership = np.sign(xs @ w + b)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=class_membership);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9A7QTEp2TgJU",
      "metadata": {
        "id": "9A7QTEp2TgJU"
      },
      "source": [
        "Plot the accuracy as a function of $t$ to see how we converged to a solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BGs2VpuJRAlV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "BGs2VpuJRAlV",
        "outputId": "b3f51f3c-22e4-42cd-fb20-5780d46674c2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAbXklEQVR4nO3de3hU9b3v8feXXAkEAiEhQIgBDSioXIy3ausFVLRVbOupYrt72bXsp1s92u7dHj32aU9tn71tu2u3ntqLtXZ7eqpUrUeppdLWYlXqJVCU+yXcTCBAEiBXMskkv/PHLOgYCJmQmayZNZ/X8+Rh1m+tJN+Vlfnwy2/91lrmnENERFLfML8LEBGR+FCgi4gEhAJdRCQgFOgiIgGhQBcRCYhMv77xuHHjXHl5uV/fXkQkJa1evbrBOVd0onW+BXp5eTmrVq3y69uLiKQkM9vd1zoNuYiIBIQCXUQkIBToIiIBoUAXEQkIBbqISED0G+hm9riZHTCz9X2sNzN72MyqzWytmc2Nf5kiItKfWHro/wUsOMn6a4EK72Mx8OPBlyUiIgPV7zx059yrZlZ+kk0WAv/HRe7D+6aZFZjZBOdcXZxqFBE5JTUH23l2dS3JdpvweWeNZ9bkgrh/3XhcWDQJqIlarvXajgt0M1tMpBdPWVlZHL61iEjf/u+bu/npqzsw87uS9yselZu0gR4z59yjwKMAlZWVyfVfpkgcdXX3sLmuhZ4k6xmmm12NbYwbmcOqr833u5QhEY9A3wNMjlou9dpE0tZjr+3kOy9t9rsMAc4syfe7hCETj0BfCtxhZkuAC4EmjZ9Lutvf3EFedgY/vHWO36WkvYpiBfoxZvYUcDkwzsxqgW8AWQDOuZ8Ay4DrgGqgHfhcoooV8cPdS9bwenXDgD6npSNM4YhsrjxzfIKqEjleLLNcFvWz3gG3x60ikSTz2rYGCkdmc3752AF93kC3Fxks326fK5LsVu8+xNNVNRw+0sVNlaXce+1ZfpckclIKdJE+/OrN3bzw7l5KRuVy/mnqbUvyU6BLWtvd2MbmfS0nXLejoY2K4pG8dPeHhrgqkVOjQJe0dvuTf2P9nuY+118+/YRP+hJJSgp0CaRwd09M2zW2dnLVjPHcPb/ihOvLC0fEsyyRhFKgS+D8YuVOvvnbjTFvf905E5g5cXQCKxIZGgp0CZyNe5vJz81k8Qen9rutGSycPWkIqhJJPAW6BEJPj+O+59ez9/ARNtY1M35ULnfOO/EwikhQKdAlEBpaQzz19nuUjhnOxILhXD1DV2hK+lGgSyC8uDZy+6B/vXo6N87REIqkJz1TVFJeV3cP978YOQlaVpjnczUi/lEPXVJOKNxNQ2vnseXmI10A/MtV05hbNsavskR8p0CXlPO5X1Tx1+2Nx7VPHqveuaQ3BbqknPcOtnPeaWO4ufLvz1XJzhzGgrNLfKxKxH8KdElKP35lOy+8c+IHX9U1dXDF9GI+cf7kE64XSVcKdElKv19fR2NbJ3PLjn+QbnnhCG6cM9GHqkSSmwJdktLmuhaunjmeH9461+9SRFKGpi1KUup2jo6ubr/LEEkpCnRJOk3tXXT3OM4tPX64RUT6pkCXpPPV37wLwLiROT5XIpJaFOiSdA61dTEiO4Obziv1uxSRlKKTouKbb7ywnhfe3Xtce0tHmCumF5Odqf6GyEAo0MU3b+xopGB4FpdNO/4xbx+ZpWmJIgOlQBdfNB3pYuv+Vm46r5RvLjzb73JEAkF/04ovqnYeBGBSwXCfKxEJDvXQZUjVt4So2nWQql2RQF84W0MrIvGiQJch9e/LNvHcmsg9WrIyjEJNTRSJGwW6DInOcA8OR31riOnj83l40RwK8rIYPTzL79JEAkOBLgm3bF0dtz/5N5yLLH+wYhzTS/L9LUokgBToknBb9rXgHHzlmukAJ5ymKCKDp0CXhHt6VQ3ZmcO4/Yoz/C5FJNA0bVESrqUjTK6u+hRJOL3LJOG6untYdEGZ32WIBJ4CXRLKOUco3EOOeugiCad3mSRUXVMHADlZGT5XIhJ8CnRJmE11zXzggT8DkJ+r8+8iiaZAl4TZc+gIEJmu+PG5ure5SKLF1G0yswXAQ0AG8Jhz7oFe68uAJ4ACb5t7nHPL4lyrpIh3aw7zP36zlsPtXQAsOLuEETnqoYskWr89dDPLAB4BrgVmAIvMbEavzb4GPO2cmwPcAvwo3oVK6qjadZDN+1qYNXk0n774NE4bm+d3SSJpIZZu0wVAtXNuB4CZLQEWAhujtnHAKO/1aOD4x9BIWlix5QDLN+wD4EefPI+MYeZzRSLpI5ZAnwTURC3XAhf22uZ/AX8wszuBEcD8E30hM1sMLAYoK9O85CB6YNlmth1oYW5ZgcJcZIjF66ToIuC/nHOlwHXAL83suK/tnHvUOVfpnKssKtL9PILEOce62iYa2zr52NxSnvvnS/wuSSTtxNJD3wNMjlou9dqifR5YAOCce8PMcoFxwIF4FCnJ743tjdz62FsAFOXrHucifoilh14FVJjZFDPLJnLSc2mvbd4D5gGY2VlALlAfz0Ilue1viVxA9IObZ3HnlboJl4gf+g1051wYuANYDmwiMptlg5ndb2Y3eJv9C/AFM3sXeAr4rHNH734t6eCx13YCcOkZReRla4qiiB9ieud5c8qX9Wr7etTrjYAGTdNYd0/k/+9xI7N9rkQkfelKUYmLznAPHzl3Amaa2SLiFwW6xEVHVze5ugGXiK802CmnpKOrmxWbD9DlDbW0dIR1i1wRnynQ5ZQ8v2YP9zy37n1t40fl+lSNiIACXU5RY1snAL+/64NkZQzDDKYUjvC5KpH0pkCXU/LCO5Fry84sydeJUJEkoUFPOSW5WRkU5+cozEWSiAJdBqT6QAv/8PO32La/lfOnjPW7HBGJokCXAVlZ3chr2xqYMXEU1587we9yRCSKxtAlZqFwN0/8dRcAv7rtQs07F0ky6qFLzN7acZAdDW1kDjPNORdJQnpXSsy2HWgFYOkdl+pkqEgSUqBLTHbUt/KtFyNPHdT9zkWSkwJdYlLfEgLg9itOV6CLJCkFusSks7sHgMunF/tciYj0RYEuMekMRwI9O0O/MiLJSu9OicmxQNfsFpGkpXenxOTokEuWeugiSUvvTonJ4fYuAM0/F0liendKTFZWNwCQn6uLi0WSlQJdYpKZYYwenkVBnh4CLZKsFOjSr7W1h1m2bh+nFeb5XYqInIQCXfq1ua4FgE9eWOZzJSJyMhoQlZP62as7eOHdyNOJFszU7XJFkpkCXU7qP/+0lcyMYVw+vUgnREWSnN6h0qfWUJi2zm7umjeVL101ze9yRKQfGkOXPj2/JjLUUjhSM1tEUoECXfrUdCRyMdEnKif7XImIxEKBLn16fVvkYiI9ak4kNSjQpU/DszPIz9FpFpFUoXerHOfnr+9k+fp9bNrXzIyJo/wuR0RipECX4zyzqoYDLSHOnjiaj86d5Hc5IhIjBboAEO7uYUlVDa2hMPuaO7hyejEP3jzb77JEZAAU6ALAu7WH+drz648tTyvJ97EaETkVCnQBYHt9GwBLFl/E7MkFmtkikoI0y0VoC4X56rNrAZgwOldhLpKiFOjCwbZOAG6cPZHTCkf4XI2InKqYAt3MFpjZFjOrNrN7+tjmE2a20cw2mNmT8S1TEumbv90AwNUzS3yuREQGo98xdDPLAB4BrgJqgSozW+qc2xi1TQVwL3CJc+6QmRUnqmCJv03e/c4vnlrocyUiMhix9NAvAKqdczucc53AEmBhr22+ADzinDsE4Jw7EN8yJVE6urrZc/gIt15YxpgRugmXSCqLJdAnATVRy7VeW7RpwDQzW2lmb5rZghN9ITNbbGarzGxVfX39qVUscfXwy9sA+HVVTT9bikiyi9dJ0UygArgcWAT8zMwKem/knHvUOVfpnKssKiqK07eWwdiyLzLcUjIq1+dKRGSwYgn0PUD0/VNLvbZotcBS51yXc24nsJVIwEuS21jXDMBH5+gSf5FUF0ugVwEVZjbFzLKBW4ClvbZ5nkjvHDMbR2QIZkcc65QEmTY+ckWonkgkkvr6DXTnXBi4A1gObAKeds5tMLP7zewGb7PlQKOZbQRWAF9xzjUmqmiJj5fW1/GXrfVcckYhGcPM73JEZJBiuvTfObcMWNar7etRrx3wZe9DUsS6PU0AfGm+euciQaArRdPUC+/s4Xdr6xg9PIvK8rF+lyMicaBAT1OPr9xFfUuI62dN8LsUEYkT3W0xjazefYidDZG7Ku5v6uCy6UV8+8ZzfK5KROJFgZ5GPvv427SEwseWS8fk+ViNiMSbAj1NdHX30BIKc9ulU/jMB8oBmFQw3N+iRCSuFOhpYpN3AVFRfg6Tx6pnLhJEOimaJvY3hwA4e9JonysRkURRDz0NPPSnbTz/TuRuDRNG654tIkGlQE8Dz6yuoau7h4/OmaThFpEA05BLwHV191B76AjXzCzhBzfPJitDh1wkqPTuDrh3ag4D6MHPImlAgR5w7Z3dAFyj54WKBJ4CPeA6uiKBnpulQy0SdHqXB1wo3ANATqaGXESCToEecPf/dgOgHrpIOtC7POAa2zrJy85g4mhd5i8SdAr0AGtq78I5uP2KMximJxKJBJ4CPcCOPpFoZI6uHxNJB3qnB1DNwXa217fyt/cic9Ary8f4XJGIDAUFegB9/okqtu5vBcAMikbm+FyRiAwFBXoANbR2smBmCYsvm8qYvGyKR+mGXCLpQIEeMLsa2jjY1snkscOZW6ahFpF0opOiAbO9PjLUMmtygc+ViMhQU6AHSGNriLt//Q4AZ00Y5XM1IjLUFOgBUrXrEC0dYSYVDNfzQkXSkMbQU5xzjj9s3M/h9k7WeNMUn/rCRbpdrkgaUqCnuN2N7fzTL1cfW87LzqBwZLaPFYmIXxToKe5geycA3/9vs7j49ELyczMZoStDRdKS3vkp7kcrqgGYUjSCiRo3F0lrOima4jq7HQCzSjVNUSTdqYeeog63d3L7k3/jnfcOc8kZhWTobooiaU899BS1sa6ZldWNVIzP5+bzy/wuR0SSgHroKeqVLfUAfGvh2ZxTOtrnakQkGaiHnqJe2XIAgEljdCJURCIU6CmoLRRm6/5Wbpw9kbEjNOdcRCIU6Cno6P1axuk+5yISRYGegvY1dZCbNYw751X4XYqIJJGYToqa2QLgISADeMw590Af230ceBY43zm3Km5VprFXt9Zz73PrCPf0HGtraO3kunMmMHp4lo+ViUiy6TfQzSwDeAS4CqgFqsxsqXNuY6/t8oG7gLcSUWi6Wr37EHsOH+GW8ye/r/2m80p9qkhEklUsPfQLgGrn3A4AM1sCLAQ29truW8B3gK/EtcI01BYK84M/bqW9q5s17x0mLzuDBz5+rt9liUiSi2UMfRJQE7Vc67UdY2ZzgcnOud+d7AuZ2WIzW2Vmq+rr6wdcbLqo2nWQx17fybJ1ddS3hLhsWpHfJYlIChj0hUVmNgx4EPhsf9s65x4FHgWorKx0g/3eQVVzsB2Ap//pYqaNz/e5GhFJFbH00PcA0QO4pV7bUfnA2cArZrYLuAhYamaV8Soy3by8OXLRkKYlishAxBLoVUCFmU0xs2zgFmDp0ZXOuSbn3DjnXLlzrhx4E7hBs1xOnXMwqWC4LhoSkQHpN9Cdc2HgDmA5sAl42jm3wczuN7MbEl1gOtqwt5lSXdIvIgMU0xi6c24ZsKxX29f72PbywZeV3kbmZNDjdIpBRAZGV4omkc5wD//6zLvsbergjGKdDBWRgVGgJ5HdjW08u7qWklG5zDuz2O9yRCTFKNCTSCgcubz/vg+fxfwZ432uRkRSjQI9iXR1RwI9O1OHRUQGTsmRJNpCYfY3hwDIztBhEZGB0yPoksCRzm4u+reXaQmFAcjLzvC5IhFJRQr0JHCwvZOWUJiPzZ3E5dOLObe0wO+SRCQFKdB9dKC5gzueXMPB9k4ArphezPWzJvpclYikKg3W+mj93ibe3nWQsXnZfPjcCVw4ZazfJYlIClMP3SetoTAP/nErAP/2sbN1IZGIDJp66D75y5Z61u9pBmD8qFyfqxGRIFAPfQjVNR3hgDc1cVNdJMzfuPdK8nP1bFARGTwF+hDp7nFc9eCrtHpTEwEyh5ke9CwicaNAHyJtnWFaQ2EWXTCZq7zL+ovzc8nL1iEQkfhQmgyB/7emlm+/uAmAWaUFXHmm7tMiIvGnk6JD4O2dh2jv7OZzl5Rz5Vm6i6KIJIZ66AlysK2Th/60lY6uHt7c2cj4UTl84/qZfpclIgGmQE+Q17bV88Qbuxk3MofMYcY1MzXMIiKJpUBPkLW1TQAs+++XUqx55iIyBDSGniDPrq4FYJSmJYrIEFGgJ4Bzjq7uHj58zgRys3QrXBEZGgr0OHPOceX3/0J7ZzenF4/0uxwRSSMK9Dhr7+xmZ0MbRfk5LLpgst/liEgaUaDH2R837gfgrnkVTBg93OdqRCSdKNDj7K/bGwB0b3MRGXKathgnO+pbWVndwKa6FqYWjaBivO5vLiJDS4EeJ999aQsvbdgHwHxd3i8iPlCgx0ljW4g5ZQX87NOVFGjuuYj4QGPocbJhbzPDszIil/pn6McqIkNPyRMnuVkZjMzRHzwi4h8l0CB0dHVz2xOraGgNcai9kynjRvhdkoikMfXQB2HP4SO8Xt1ATuYwFsws4cPnTvC7JBFJY+qhD8KKzQcAuPPKCubP0O1xRcRf6qEPwm/X1gEwtUhDLSLiP/XQByAU7mbD3maciywfautkwcwSphbpJlwi4j8F+gD875er+eGK6ve1fWjaOJ+qERF5PwX6AOxv7mDsiGx+cPPsY22zJxf4WJGIyN/FFOhmtgB4CMgAHnPOPdBr/ZeB24AwUA/8o3Nud5xr9UVHVzc3PrKS/c0dtIW6KSvM47JpRX6XJSJynH4D3cwygEeAq4BaoMrMljrnNkZttgaodM61m9kXge8CNyei4KFW3xJi874WLjmjkNOLRvKB0zXEIiLJKZYe+gVAtXNuB4CZLQEWAscC3Tm3Imr7N4FPxbPIofbIimp21LcB0NzRBcCnLjyNa8/RPHMRSV6xBPokoCZquRa48CTbfx74/YlWmNliYDFAWVlZjCUOrY6ubr63fAujcjPJz43cZKuieCQzJo7yuTIRkZOL60lRM/sUUAlcdqL1zrlHgUcBKisrXTy/d7xsr28F4CvXTOcfLi73txgRkQGIJdD3ANEPxyz12t7HzOYD9wGXOedC8Slv6H3/D1sBKNHj40QkxcQS6FVAhZlNIRLktwC3Rm9gZnOAnwILnHMH4l5lAoS7e+h2x/+R0BYKM6lguB5SISIpp99Ad86FzewOYDmRaYuPO+c2mNn9wCrn3FLge8BI4BkzA3jPOXdDAuselPqWEFf8xyu0hsInXD/vzGK8/RARSRkxjaE755YBy3q1fT3q9fw415VQtYfaaQ2Fuem80hPe8vaK6eqdi0jqCfyVon/evJ/HX9/1vramI5GpiJ+onMwFU8b6UJWISPwF/m6LL7yzl6pdBznS1X3sIztzGJdNK2J6Sb7f5YmIxE0ge+iNrSF+t66O7h7H5roWzigeyW+++AG/yxIRSahABvqSqhq+t3zLseXrZ030sRoRkaGRkoHunKP20BFOMOsQiJz0zMkcxlv/cx4Ao7wrPkVEgiwlA/3xlbv41osbT7rNxNG5FORlD1FFIiL+S8lArz3UzvCsDL5949l9bjNtvE54ikh6SblAf7qqhl+s3MX4UTl8/LxSv8sREUkaKRfoBXlZXHdOCRdNLfS7FBGRpJJygX71zBKunlnidxkiIkkn8BcWiYikCwW6iEhAKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgFhrq9bFib6G5vVA7tP8dPHAQ1xLCcVaJ/Tg/Y5PQxmn09zzhWdaIVvgT4YZrbKOVfpdx1DSfucHrTP6SFR+6whFxGRgFCgi4gERKoG+qN+F+AD7XN60D6nh4Tsc0qOoYuIyPFStYcuIiK9KNBFRAIi5QLdzBaY2RYzqzaze/yu51SZ2WQzW2FmG81sg5nd5bWPNbM/mtk2798xXruZ2cPefq81s7lRX+sz3vbbzOwzfu1TrMwsw8zWmNmL3vIUM3vL27dfm1m2157jLVd768ujvsa9XvsWM7vGnz2JjZkVmNmzZrbZzDaZ2cVBP85m9iXv93q9mT1lZrlBO85m9riZHTCz9VFtcTuuZnaema3zPudhM7N+i3LOpcwHkAFsB6YC2cC7wAy/6zrFfZkAzPVe5wNbgRnAd4F7vPZ7gO94r68Dfg8YcBHwltc+Ftjh/TvGez3G7/3rZ9+/DDwJvOgtPw3c4r3+CfBF7/U/Az/xXt8C/Np7PcM79jnAFO93IsPv/TrJ/j4B3Oa9zgYKgnycgUnATmB41PH9bNCOM/AhYC6wPqotbscVeNvb1rzPvbbfmvz+oQzwB3gxsDxq+V7gXr/ritO+vQBcBWwBJnhtE4At3uufAouitt/irV8E/DSq/X3bJdsHUAq8DFwJvOj9sjYAmb2PMbAcuNh7neltZ72Pe/R2yfYBjPbCzXq1B/Y4e4Fe44VUpnecrwnicQbKewV6XI6rt25zVPv7tuvrI9WGXI7+ohxV67WlNO9PzDnAW8B451ydt2ofMN573de+p9rP5D+BrwI93nIhcNg5F/aWo+s/tm/e+iZv+1Ta5ylAPfALb5jpMTMbQYCPs3NuD/AfwHtAHZHjtppgH+ej4nVcJ3mve7efVKoFeuCY2UjgN8Ddzrnm6HUu8l9zYOaVmtlHgAPOudV+1zKEMon8Wf5j59wcoI3In+LHBPA4jwEWEvnPbCIwAljga1E+8OO4plqg7wEmRy2Xem0pycyyiIT5r5xzz3nN+81sgrd+AnDAa+9r31PpZ3IJcIOZ7QKWEBl2eQgoMLNMb5vo+o/tm7d+NNBIau1zLVDrnHvLW36WSMAH+TjPB3Y65+qdc13Ac0SOfZCP81HxOq57vNe9208q1QK9CqjwzpZnEzmBstTnmk6Jd8b658Am59yDUauWAkfPdH+GyNj60fZPe2fLLwKavD/tlgNXm9kYr2d0tdeWdJxz9zrnSp1z5USO3Z+dc58EVgA3eZv13uejP4ubvO2d136LNztiClBB5ARS0nHO7QNqzGy61zQP2EiAjzORoZaLzCzP+z0/us+BPc5R4nJcvXXNZnaR9zP8dNTX6pvfJxVO4STEdURmhGwH7vO7nkHsx6VE/hxbC7zjfVxHZOzwZWAb8CdgrLe9AY94+70OqIz6Wv8IVHsfn/N732Lc/8v5+yyXqUTeqNXAM0CO157rLVd766dGff593s9iCzGc/fd5X2cDq7xj/TyR2QyBPs7AN4HNwHrgl0RmqgTqOANPETlH0EXkL7HPx/O4ApXez2878EN6nVg/0Ycu/RcRCYhUG3IREZE+KNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgHx/wFROpfMDHK6/wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(2)\n",
        "plt.plot(accuracy);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f277317c",
      "metadata": {
        "id": "f277317c"
      },
      "source": [
        "## 2.5 Perceptron (4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19a962a5",
      "metadata": {
        "id": "19a962a5"
      },
      "source": [
        "The perceptron model defines a classifier of the form $\\hat{y}(x) = \\text{sgn}(\\theta^T x)$ where $x \\in \\mathbb{R}^{d+1}$ is some input vector (with $\\text{x}_0=0$ to include a bias term) and $\\theta \\in \\mathbb{R}^{d+1}$ some trainable weight vector and $\\text{sgn}(\\cdot) $ is the sign function. The class labels are assumed to come from the set $\\lbrace -1, 1 \\rbrace$ due to the use of the $\\text{sgn}$ function.\n",
        "\n",
        "The basic idea of the model is to achieve that $\\theta^T x > 0$ holds for samples with class label $y=1$ and, analogously, that $\\theta^T \\mathbf{x} < 0$ holds for samples with class label $y = -1$. Conveniently, both criteria can be summarized by the following rule:\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "y \\cdot \\theta^T x > 0\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e00613",
      "metadata": {
        "id": "81e00613"
      },
      "source": [
        "In this exercise, we will use the perceptron model on the same dataset as last time. But, first we have to adapt the labels to be $y = \\{-1,1\\}."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563b1577",
      "metadata": {
        "id": "563b1577"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "xs, ys = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "ys[ys == 0] = -1  # make the negative class labels == -1\n",
        "plt.figure(0)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IlmTUOottVKF",
      "metadata": {
        "id": "IlmTUOottVKF"
      },
      "source": [
        "Here is a helper function to plot the data and the decision of the perceptron based on $\\theta$. Let's give an initial value of $\\theta = [1 \\, 1 \\, 1]^T$ and see what the decision looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LkkAczk2tgr4",
      "metadata": {
        "id": "LkkAczk2tgr4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def plot_perceptron(xs,w,t):\n",
        "  \"\"\" input: xs data\n",
        "             w = parameters [b w1 w2]\n",
        "             t = figure handle\n",
        "  \"\"\"\n",
        "  b, w1, w2 = w\n",
        "  xs_hom = np.concatenate([np.ones(shape=[len(xs), 1]), xs], axis=1)\n",
        "  yy = np.linspace(np.min(xs[:, 1]), np.max(xs[:, 1]), 20)\n",
        "  xx = (w2 * yy + b)/(-w1)\n",
        "  plt.figure(t)\n",
        "  plt.plot(xx, yy, color=\"orange\")\n",
        "  class_membership = np.sign(xs_hom @ w)\n",
        "  plt.scatter(x=xs[:, 0], y=xs[:, 1], c=class_membership)\n",
        "\n",
        "t = plt.figure()\n",
        "w0 = np.ones(2 + 1)\n",
        "plot_perceptron(xs,w0,t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7P13XIh8wYMy",
      "metadata": {
        "id": "7P13XIh8wYMy"
      },
      "source": [
        "**2.5.1** *Implement the* perceptron *algorithm from the Lecture Notes. We also provide you with a helper function to plot the decision of the perceptron. Include in the function an option to plot the decision of the perceptron every $t=10$ steps.* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2pyTfLKMFfW",
      "metadata": {
        "id": "a2pyTfLKMFfW"
      },
      "outputs": [],
      "source": [
        "def perceptron_algo(xs_hom, w, ys, pl):\n",
        "  \"\"\" input:  xs_hom = [n x d+1] augmented xs\n",
        "              w0 = initial [b w1 w2]\n",
        "              ys = [n x 1] labels\n",
        "      output: w = optimized w\n",
        "              accuracy = list of accuracy over t\n",
        "  \"\"\"\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  if (i % 10 == 0) and (pl):\n",
        "    plot_perceptron(xs,w,i)\n",
        "return w, accuracy, i"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y9zGQfKEwenz",
      "metadata": {
        "id": "y9zGQfKEwenz"
      },
      "source": [
        "**2.5.2** *Apply the perceptron algorithm one pass over the data. Plot the perceptron decision every 10 iterations, and plot the accuracy over time.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uXLUnrvKMKSt",
      "metadata": {
        "id": "uXLUnrvKMKSt"
      },
      "outputs": [],
      "source": [
        "w0 = np.ones(2 + 1)\n",
        "xs_hom = np.concatenate([np.ones(shape=[len(xs), 1]), xs], axis=1)\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7gmWtNxTA1qh",
      "metadata": {
        "id": "7gmWtNxTA1qh"
      },
      "source": [
        "Recall from the lecture notes the *perceptron loss* which is given by\n",
        "\n",
        "<br>\n",
        "\n",
        "$\\begin{align}\n",
        "\\ell_{\\text{perceptron}} (y_i, \\hat{y}_i)  &= \\text{max} (0, -y_i \\, \\theta^T x_i)\n",
        "\\end{align}$\n",
        "\n",
        "<br>\n",
        "and can be turned into an average loss by summing and taking the negative\n",
        "\n",
        "$\\begin{align}\n",
        "L(\\theta)  &= - \\frac{1}{n} \\sum_{i=1}^n \\, \\text{max} (0, -y_i \\, \\theta^T x_i)\n",
        "\\end{align}$\n",
        "\n",
        "and rewritten\n",
        "\n",
        "$\\begin{align}\n",
        "L(\\theta)  &= - \\frac{1}{|\\mathcal{M}|} \\sum_{i \\in \\mathcal{M}} \\, y_i \\, \\theta^T x_i\n",
        "\\end{align}$\n",
        "\n",
        "where $\\mathcal{M}$ are the set of samples that are misclassified $y_i \\theta^T x_i < 0$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PovHU-91wkgw",
      "metadata": {
        "id": "PovHU-91wkgw"
      },
      "source": [
        "**2.5.3** *Can you find a connection between the* average perceptron loss *above, the update in the perceptron algorithm, and stochastic gradient descent? Why are the two negative signs in the average loss function necessary?* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wudx4hLSG5Eh",
      "metadata": {
        "id": "Wudx4hLSG5Eh"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2efbd356",
      "metadata": {
        "id": "2efbd356"
      },
      "source": [
        "## 2.6 SVM (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce41c6f",
      "metadata": {
        "id": "cce41c6f"
      },
      "source": [
        "**SK-Learn** features an [implementation of SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), which itself is based on [libsvm](https://www.csie.ntu.edu.tw/~cjlin/libsvm/). It uses the dual formulation and relies on an SMO-like solver to optimize $\\theta$. One might argue that code underlying `libsvm` and thus `sklearn.svm.SVC` has been used to make more ML predictions than any other.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's build our dataset once again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f741cfc8",
      "metadata": {
        "id": "f741cfc8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "xs, ys = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "ys[ys == 0] = -1  # make the negative class labels == -1\n",
        "plt.figure(0)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6008c875",
      "metadata": {
        "id": "6008c875"
      },
      "source": [
        "**2.6.1** *Using `sklearn.svm`, fit an SVM model to the training data and report the accuracy.* $\\color{red}{\\text{(1 point)}}$\n",
        "\n",
        "*hint: training an SVM in `sklearn` works very similar to linear regression. you can look it up again.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caRvzE17NxNM",
      "metadata": {
        "id": "caRvzE17NxNM"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"Accuracy: {}%\".format(100.0 * np.mean(ys == ys_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_SDD_ykYzM8g",
      "metadata": {
        "id": "_SDD_ykYzM8g"
      },
      "source": [
        "Here is a handy plotting function to visualize the performance of your SVM. Use it to plot the results of your trained model. What do you see? Can you identify the margin hyperplanes? The support vectors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kI1YVkf4zg5q",
      "metadata": {
        "id": "kI1YVkf4zg5q"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "def plot_svm(model, xs, ys, h):\n",
        "  \"\"\"\n",
        "  input:  model: sklearn.svm.SVC model\n",
        "          theta: trained weights\n",
        "          xs:    data\n",
        "          ys:    labels\n",
        "          h:     figure handle\n",
        "  output: plots SVM for 3.9\n",
        "  \"\"\"\n",
        "  # get the separating hyperplane\n",
        "  theta = model.coef_[0]\n",
        "  a = -theta[0] / theta[1]\n",
        "  xx = np.linspace(-1.5, 1)\n",
        "  yy = a * xx - (model.intercept_[0]) / theta[1]\n",
        "  margin = 1 / np.sqrt(np.sum(model.coef_ ** 2))\n",
        "  yy_neg = yy - np.sqrt(1 + a ** 2) * margin\n",
        "  yy_pos = yy + np.sqrt(1 + a ** 2) * margin\n",
        "\n",
        "  # plot the line, the points, and the nearest vectors to the plane\n",
        "  plt.figure(h);\n",
        "  plt.plot(xx, yy, \"k-\")\n",
        "  plt.plot(xx, yy_neg, \"k--\")\n",
        "  plt.plot(xx, yy_pos, \"k--\")\n",
        "  plt.scatter(x=xs[:, 0], y=xs[:, 1], c=ys);\n",
        "  YY, XX = np.meshgrid(yy, xx)\n",
        "  xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "  Z = model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "  # Put the result into a contour plot\n",
        "  plt.contourf(XX, YY, Z, cmap=cm.get_cmap(\"RdBu\"), alpha=0.5, linestyles=[\"-\"])\n",
        "  plt.xlim([-1.6,1.1])\n",
        "  plt.ylim([-0.24,1.77])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_svm(model,xs,ys,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gISrSVbozibz",
      "metadata": {
        "id": "gISrSVbozibz"
      },
      "source": [
        "How can we make the margin bigger? Try playing with the `C` hyperparameter in `sklearn.svm.SVC`.\n",
        "\n",
        "<br>\n",
        "\n",
        "**2.6.2** *Write some code to fit two or more SVMs that allow for more slack than the default settings. What are the consequences of this? How did you accomplish it and what was the result? What happened to the margin? Why?* $\\color{red}{\\text{(2 points)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cDTnvVyZOMTT",
      "metadata": {
        "id": "cDTnvVyZOMTT"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbnA-8l4-Q4S",
      "metadata": {
        "id": "fbnA-8l4-Q4S"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_sp8xNY8w3B3",
      "metadata": {
        "id": "_sp8xNY8w3B3"
      },
      "source": [
        "## 2.7 Primal-Dual & Constrained Problems (2 pts + 3 bonus pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ee21b8f",
      "metadata": {
        "id": "9ee21b8f"
      },
      "source": [
        "Assume that we want to have some arbitrary point $\n",
        "{p} = [p_1\\ p_2\\ p_3]^T \\in \\mathbb{R}^3$ and some ball $S$ centered on the point ${q} = [q_1\\ q_2\\ q_3]^T$ with radius $R$. We are interested in finding the point on $S$ that is closest to ${p}$ in terms of the *squared Euclidean* distance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e951008d",
      "metadata": {
        "id": "e951008d"
      },
      "source": [
        "This *constrained optimization* problem can be phrased as follows:\n",
        "\\begin{align}\n",
        "\\min_{{x}} \\lVert {x} - {p} \\rVert_2^2 \\\\\n",
        "\\text{subject to:} \\lVert {x} - {q} \\rVert_2^2 \\leq R^2\n",
        "\\end{align}\n",
        "\n",
        "which can be rewritten:\n",
        "\\begin{align}\n",
        "\\min_{{x}} f({x})=\\lVert {x} - {p} \\rVert_2^2 \\\\\n",
        "\\text{subject to: } g({x})= \\lVert {x} - {q} \\rVert_2^2 - R^2 \\leq 0\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "The Lagrangian primal problem is:\n",
        "\\begin{align}\n",
        "\\min_{x \\in \\mathbb{R}} \\mathcal{\\psi_P}(x)  \\\\\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "and the Lagrangian dual problem is then given by:\n",
        "\\begin{align}\n",
        "\\max_{\\alpha \\in \\mathbb{R}} \\mathcal{\\psi_D}(\\alpha)  \\\\\n",
        "\\text{subject to: } \\alpha \\geq 0\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "where $\\psi_\\mathcal{D}(\\alpha) = \\min_{{x}} \\mathcal{L}({x}, \\alpha)$ and $\\psi_\\mathcal{P}(\\alpha) = \\max_{{\\alpha}} \\mathcal{L}({x}, \\alpha)$, and $\\mathcal{L}({x}, \\alpha) = f({x}) + \\alpha g({x})$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bmmIuFPrw8an",
      "metadata": {
        "id": "bmmIuFPrw8an"
      },
      "source": [
        "**2.7.1** *Give the expression for the Lagrangian $\\mathcal{L}$.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q47omjNDdlwI",
      "metadata": {
        "id": "q47omjNDdlwI"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6XAVytTuxAXW",
      "metadata": {
        "id": "6XAVytTuxAXW"
      },
      "source": [
        "**2.7.2** *Derive the gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}$ and set it equal to zero $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}} = 0$.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36d0c56",
      "metadata": {
        "id": "e36d0c56"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a7ebdde",
      "metadata": {
        "id": "0a7ebdde"
      },
      "source": [
        "**2.7.3** *Find the saddle point ${x}^\\star$, where $\\frac{\\partial \\mathcal{L}}{\\partial {x}} = 0$ in terms of $p$, $q$, and $\\alpha$*. $\\color{blue}{\\text{(1 bonus point)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8d2263",
      "metadata": {
        "id": "0c8d2263"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b5befd",
      "metadata": {
        "id": "21b5befd"
      },
      "source": [
        "**2.7.4** *Finally, show that the dual in standard form, expressed in terms of $\\alpha, {q}, {p}$ and $R$ is*\n",
        "\n",
        "\n",
        "$\\begin{align}\n",
        "\\max_{{\\alpha}} \\frac{\\alpha}{1+\\alpha} \\; \\lVert q-p \\rVert_2^2 - \\alpha R^2\\\\\n",
        "\\text{s.t.} \\quad \\alpha \\geq 0 \\\\\n",
        "\\end{align}$\n",
        "\n",
        "*To do this, you have to plug in the solution ${x}^\\star$ that you found above.* $\\color{blue}{\\text{(2 bonus points)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e9f215b",
      "metadata": {
        "id": "6e9f215b"
      },
      "source": [
        "$\\color{green}{\\text{YOUR ANSWER HERE}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3XPAewjxPA8",
      "metadata": {
        "id": "I3XPAewjxPA8"
      },
      "source": [
        "## 2.8 K-Medoids Clustering (5 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc7ef85a",
      "metadata": {
        "id": "cc7ef85a"
      },
      "source": [
        "Assume that we have a dataset $\\mathcal{D} = \\lbrace \\mathbf{x}_i \\rbrace_{i=1}^N$ *without* any labels. We are interested in finding a certain number of clusters (called $K$) around which other points tend to \"gather\". One simple method to achieve this is called **K-Medoids**. This method entails that we have to pick $K$ upfront, thereby we express our prior knowledge on how many clusters we believe to have in the data.\n",
        "\n",
        "Suppose we start out with some cluster centres $\\mathbf{S}_k$ ($k=1,\\ldots,K$) that are initialized by randomly picking points (without repetition).\n",
        "\n",
        "A simple way to solve this problem is by using *Voronoi iterarion* also known as *Lloyd's Algorithm*. It comprises two steps:\n",
        "1. *Assign* - Map each point $\\mathbf{x} \\in \\mathcal{D}$ to the closest cluster w.r.t. some distance (Euclidean in this case)\n",
        "2. *Update* - For each cluster $\\mathbf{S}_k$, recalculate $x_{m_k}$ by finding the point (the so-called medoid point) within the cluster $k$ that has the minimum distance w.r.t. the other points of cluster $k$\n",
        "3. Repeat step 1.\n",
        "\n",
        "Your task will be to program the three functions `initialize_cluster_centres_from_random_points`, `assign` and `update`. You can use `scipy.spatial.distance.cdist` to compute the pairwise distances."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afab276",
      "metadata": {
        "id": "3afab276"
      },
      "source": [
        "**2.8.1** *Program the function to initialize $K$ clusters* ```initialize_cluster_centres_from_random_points(xs, K)```. $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xr7mA8y2YeWt",
      "metadata": {
        "id": "xr7mA8y2YeWt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_cluster_centres_from_random_points(xs, K):\n",
        "    \"\"\"\n",
        "    Randomly samples K points from 'xs'\n",
        "    Returns\n",
        "      An np.ndarray of shape [K, d] containing the initial medoids\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810ed400",
      "metadata": {
        "id": "810ed400"
      },
      "source": [
        "**2.8.2** *Program the function to assign a label corresponding to one the $z_i \\in \\{1,\\ldots,K\\}$ clusters to each datapoint* $x_i$ ```closest_cluster(xs, m)```. $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__pUfMMMYkbg",
      "metadata": {
        "id": "__pUfMMMYkbg"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def assign(xs, m):\n",
        "    \"\"\"\n",
        "    Computes a vector of cluster indices for each point in 'xs'. Each point gets assigned to the closest cluster.\n",
        "      xs: An np.ndarray of shape [N, d] where d is the dimension of the points\n",
        "      m: An np.ndarray of shape [K, d] where K is the number of clusters\n",
        "\n",
        "    Returns:\n",
        "      An np.ndarray of shape [N]\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358d1c63",
      "metadata": {
        "id": "358d1c63"
      },
      "source": [
        "**2.8.3** *Program a function  ```medoid_point(pts)``` to returm the medoid $m_k$ of a given set of points* $S_k$. $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_JNG1m3YwAN",
      "metadata": {
        "id": "2_JNG1m3YwAN"
      },
      "outputs": [],
      "source": [
        "def medoid_point(pts):\n",
        "    \"\"\"\n",
        "    Computes the medoid point (w.r.t. the Euclidean distance)\n",
        "    pts: A np.ndarray of shape [N, d]\n",
        "\n",
        "    Returns:\n",
        "      An np.ndarray of shape [d] (the medoid)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0513336b",
      "metadata": {
        "id": "0513336b"
      },
      "source": [
        "**2.8.4** *Program a function  ```update(xs, m, z)``` that assigns new medoids that minimize the distance to members in each cluster.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SBBbiaNsY389",
      "metadata": {
        "id": "SBBbiaNsY389"
      },
      "outputs": [],
      "source": [
        "def update(xs, m, z):\n",
        "    \"\"\"\n",
        "        xs: An np.ndarray of shape [N, d] where d is the dimension of the points\n",
        "        m: An np.ndarray of shape [K, d] where K is the number of clusters\n",
        "        z: An np.ndarray of shape [N] that maps each point to a cluster index\n",
        "\n",
        "    Returns:\n",
        "       An np.ndarray of shape [K, d]\n",
        "    \"\"\"\n",
        "    K, d = m.shape\n",
        "    new_m = np.zeros_like(m)\n",
        "    for k in range(K):\n",
        "      # YOUR CODE HERE\n",
        "    return new_m"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27989a67",
      "metadata": {
        "id": "27989a67"
      },
      "source": [
        "Now, let's apply the functions on our favorite dataset. First, plot the unassigned points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0124c1b2",
      "metadata": {
        "id": "0124c1b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "xs,_ = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep=.65)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1v3res8pOhCt",
      "metadata": {
        "id": "1v3res8pOhCt"
      },
      "source": [
        "Now, pick some initial medoids. We will use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47da3899",
      "metadata": {
        "id": "47da3899"
      },
      "outputs": [],
      "source": [
        "K = 2\n",
        "m = initialize_cluster_centres_from_random_points(xs, K)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1])\n",
        "plt.scatter(x=m[:, 0], y=m[:, 1], marker=\"^\", linewidth=5);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cdd183",
      "metadata": {
        "id": "b6cdd183"
      },
      "source": [
        "Apply the assignment step to get an initial clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "neaP-UWBOfx-",
      "metadata": {
        "id": "neaP-UWBOfx-"
      },
      "outputs": [],
      "source": [
        "z = assign(xs, m)\n",
        "plt.scatter(x=xs[:, 0], y=xs[:, 1], c=z)\n",
        "plt.scatter(x=m[:, 0], y=m[:, 1], marker=\"^\", linewidth=5);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218EYKzxPlAE",
      "metadata": {
        "id": "218EYKzxPlAE"
      },
      "source": [
        "**2.8.5** *Iterate the* update *and* assign *steps $t$ times, plotting the result each step.* $\\color{red}{\\text{(1 point)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "swUgxIhHZAUc",
      "metadata": {
        "id": "swUgxIhHZAUc"
      },
      "outputs": [],
      "source": [
        "t = 3\n",
        "for i in range(t):\n",
        "  # YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
